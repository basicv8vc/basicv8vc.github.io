<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.97.3"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Yet Another Blog</title><meta name=keywords content="Blog,Portfolio,PaperMod"><meta name=description content="Hello World"><meta name=author content><link rel=canonical href=https://basicv8vc.github.io/><link crossorigin=anonymous href=/assets/css/stylesheet.min.ed52a04cba0843fef8297e018b15e8a32a989ea4415133cb8bf77414d3815f7b.css integrity="sha256-7VKgTLoIQ/74KX4BixXooyqYnqRBUTPLi/d0FNOBX3s=" rel="preload stylesheet" as=style><link rel=icon href=https://basicv8vc.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://basicv8vc.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://basicv8vc.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://basicv8vc.github.io/apple-touch-icon.png><link rel=mask-icon href=https://basicv8vc.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://basicv8vc.github.io/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:title" content="Yet Another Blog"><meta property="og:description" content="Hello World"><meta property="og:type" content="website"><meta property="og:url" content="https://basicv8vc.github.io/"><meta property="og:image" content="https://basicv8vc.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://basicv8vc.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Yet Another Blog"><meta name=twitter:description content="Hello World"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Yet Another Blog","url":"https://basicv8vc.github.io/","description":"Hello World","thumbnailUrl":"https://basicv8vc.github.io/favicon.ico","sameAs":["https://github.com/basicv8vc","https://www.zhihu.com/people/basicv8vc","https://twitter.com/b4s1cv8vc1"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://basicv8vc.github.io/ accesskey=h title="basicv8vc (Alt + H)">basicv8vc</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://basicv8vc.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>Hi</h1></header><div class=entry-content><p></p></div><footer class=entry-footer><div class=social-icons><a href=https://github.com/basicv8vc target=_blank rel="noopener noreferrer me" title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a><a href=https://www.zhihu.com/people/basicv8vc target=_blank rel="noopener noreferrer me" title=Zhihu><svg class="svg-icon" style="width:2em;height:2em;fill:currentColor;overflow:hidden" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M351.791182 562.469462h192.945407c0-45.367257-21.3871-71.939449-21.3871-71.939449L355.897709 490.530013c3.977591-82.182744 7.541767-187.659007 8.816806-226.835262h159.282726s-.86367-67.402109-18.578124-67.402109-279.979646.0-279.979646.0 16.850783-88.141456 39.318494-127.053698c0 0-83.60514-4.510734-112.121614 106.962104S81.344656 355.077018 76.80834 367.390461s24.62791 5.832845 36.941354.0c12.313443-5.832845 68.050885-25.924439 84.252893-103.69571h86.570681c1.165546 49.28652 4.596691 200.335724 3.515057 226.835262H109.86113c-25.275663 18.147312-33.701566 71.939449-33.701566 71.939449H279.868105c-8.497535 56.255235-23.417339 128.763642-44.275389 167.210279-33.05279 60.921511-50.55235 116.65793-169.802314 212.576513.0.0-19.442818 14.257725 40.829917 9.073656 60.273758-5.185093 117.305683-20.739347 156.840094-99.807147 20.553105-41.107233 41.805128-93.250824 58.386782-146.138358l-.055259.185218 167.855986 193.263655s22.035876-51.847855 5.832845-108.880803L371.045711 650.610918l-42.1244 31.157627-.045025.151449c11.69946-41.020252 20.11206-81.5749 22.726607-116.858498C351.665315 564.212152 351.72876 563.345412 351.791182 562.469462z"/><path d="M584.918753 182.033893v668.840094h70.318532l28.807093 80.512708 121.875768-80.512708h153.600307L959.520453 182.033893h-374.6017zM887.150192 778.934538h-79.837326l-99.578949 65.782216-23.537066-65.782216h-24.855084L659.341766 256.673847h227.807403V778.934538z"/></svg></a><a href=https://twitter.com/b4s1cv8vc1 target=_blank rel="noopener noreferrer me" title=Twitter><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></a></div></footer></article><article class=post-entry><header class=entry-header><h2>面向PyTorch用户的JAX简易教程[3]: 如何通过pmap轻松实现数据并行</h2></header><div class=entry-content><p>背景 在上一篇文章中，我们学习了如何使用JAX+Flax+Optax训练神经网络。但是考虑到每块Cloud TPU上有8个core/device，而我们只用了一个device，
好在我们的模型规模没有夸张到一张卡放不下，很自然的想到使用数据并行 (data parallelism, DP) 的方式来训练模型。
数据并行 ：假设有\( N \)张卡，每张卡都保存一个模型，每一次迭代（iteration/step）都将batch数据分割成\( N \)个等大小的micro-batch，每张卡根据拿到的micro-batch数据独立计算梯度，然后调用AllReduce计算梯度均值，每张卡再独立进行参数更新。
数据并行流程 注意，本文的数据并行仅限在单机多卡环境，以后如果有多机资源会进行update。
pmap+jax.lax.p* 在单机多卡上轻松实现数据并行 pmap JAX中的pmap (parallel map) 让数据并行的实现方式异常简单，先来看一个简单的pmap示例，
import jax from jax import pmap, numpy as jnp key = jax.random.PRNGKey(0) # 定义一个函数，做向量点积 def f(x, y): return jnp.dot(x, y) # 创建两个向量x, y key, init_key1, init_key2 = jax.random.split(key, 3) x = jax.random.normal(init_key1, (10, )) y = jax.random.normal(init_key2, (10, )) x....</p></div><footer class=entry-footer><span title="2022-07-26 23:17:00 +0000 +0000">July 26, 2022</span>&nbsp;·&nbsp;4 min</footer><a class=entry-link aria-label="post link to 面向PyTorch用户的JAX简易教程[3]: 如何通过pmap轻松实现数据并行" href=https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-3/></a></article><article class=post-entry><header class=entry-header><h2>面向PyTorch用户的JAX简易教程[2]: 如何训练一个神经网络</h2></header><div class=entry-content><p>背景 上一篇文章我们学习了JAX的基本知识，主要是几个关键词: NumPy API、transformations、XLA。这一篇来点实际的，看下如何训练一个神经网络，我们先回忆下用PyTorch训练神经网络，都需要哪几步：
实现网络模型 实现数据读取流程 使用优化器/调度器更新模型参数/学习率 实现模型训练和验证流程 下面我们就以在MNIST数据集上训练一个MLP为例，看下在JAX中如何实现上面的流程。
NumPy API实现网络模型 MNIST是一个10分类问题，每张图片大小是 28 * 28=784 ，我们设计一个简单的MLP网络，
一个四层MLP (包含输入层) import jax from jax import numpy as jnp from jax import grad, jit, vmap # 创建 PRNGKey (PRNG State) key = jax.random.PRNGKey(0) ## 创建模型参数, 去除输入层，实际上三层Linear，每层都包含一组(w, b)，共三组参数 def random_layer_params(m, n, key, scale=1e-2): """ A helper function to randomly initialize weights and biases for a dense neural network layer """ w_key, b_key = jax....</p></div><footer class=entry-footer><span title="2022-07-23 00:07:00 +0000 +0000">July 23, 2022</span>&nbsp;·&nbsp;9 min</footer><a class=entry-link aria-label="post link to 面向PyTorch用户的JAX简易教程[2]: 如何训练一个神经网络" href=https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-2/></a></article><article class=post-entry><header class=entry-header><h2>面向PyTorch用户的JAX简易教程[1]: JAX介绍</h2></header><div class=entry-content><p>背景 前几天申请参加了Google TRC项目，TPU VM的配置相当可以，但是PyTorch/XLA做数据并行时的体验却并不那么丝滑，考虑到Google一直力推TPU+JAX的组合，所以决定学习下JAX。
JAX简介 什么是JAX？
官方在GitHub README中是这么介绍的: JAX is Autograd and XLA, brought together for high-performance machine learning research.
在Description中写的是: Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more.
在JAX官方文档又是这么介绍的: JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.
总结一下，有几个关键词：Autograd、XLA、NumPy和composable transformations。
XLA 先来说XLA，这个我了解的最少，所以介绍起来最简单，XLA (Accelerated Linear Algebra)是Google为TensorFlow设计的一款编译器，主打JIT (Just-in-Time)编译和跨设备（CPU/GPU/TPU）执行，所以JAX介绍中凡是涉及到JIT、high-performance、CPU/GPU/TPU，都指的是XLA。
NumPy NumPy就不用提了，Python生态下只要涉及到数据分析/机器学习/数值计算中对数组/tensor进行操作，都离不开它，不夸张的说，NumPy API已经成为了数组/tensor操作的半个工业标准，包括各家深度学习框架中对tensor操作的函数接口也都是尽量靠近NumPy，JAX则更夸张，jax.numpy重新实现一套了NumPy API ，让用户从NumPy无缝切入JAX：
from jax import numpy as jnp Autograd 这里的Autograd是哈佛大学HIPS实验室在14年开始开发的一款自动微分框架，特点是可以对Python/NumPy函数进行高阶求导，直接看个例子，一个简单的函数 f(x) ，顺便求一下一阶、二阶、三阶导函数：...</p></div><footer class=entry-footer><span title="2022-07-21 01:31:00 +0000 +0000">July 21, 2022</span>&nbsp;·&nbsp;4 min</footer><a class=entry-link aria-label="post link to 面向PyTorch用户的JAX简易教程[1]: JAX介绍" href=https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-1/></a></article><article class=post-entry><header class=entry-header><h2>Sentence-BERT: 如何通过对比学习得到更好的句子向量表示</h2></header><div class=entry-content><p>背景 Sentence-BERT是对句子进行向量表示的一项经典工作，论文延伸出来的sentence-transformers 项目，在GitHub上已经收获了8.1k个star，今天重读下论文。
Introduction 句子的向量表示，也就是sentence embedding，是利用神经网络对句子进行编码，得到的固定长度向量，我们希望这个向量包含了句子的”语义信息“：
句子向量表示 句子向量可以应用于NLP领域的方方面面，我们暂时将目光聚焦到文本语义相似度检测 (semantic textual similarity, STS )任务上：给定两个句子，判断两个句子在语义层面的相似程度，相似程度可以是连续值([0, 1])也可以是离散值 (0-5)。
前BERT时代有不少出彩的工作，咱们就先略去不表了，直接看BERT是怎么做的，本身BERT模型的输入就包含两个序列，所以天然适合处理STS任务，将两个句子拼接：
[CLS] sentence 1 [SEP] sentence 2 [SEP] 直接作为BERT的输入，然后取最后一层的[CLS]向量或者所有token向量的mean/max啥的，再接一个简单的MLP即可。剩下的就是找个数据集进行fine-tune吧。
我们将这种方式称为"Cross-Encoder"，因为两个句子的token可以交互，有利于学习到句子对之间的相似性。
如果你的的任务也像STS这样，句子对(sentence pair)的关系已经固定了，只需要判断句子对的关系（比如相似程度），那么Cross-Encoder非常适合你，但是，如果你的任务是从\( N \)个句子中找出最相似的两个句子，或者找出和句子\( q \)最相似的句子，那么Cross-Encoder就面临一个计算量的问题。
\( N \) 个句子两两组合，有多少种情况？ \( \frac{N\cdot (N-1)}{2} \)
如果\( N = 10\)，结果是45 如果\( N = 100\)，结果是4950 如果\( N = 1000\)，结果是49995000 …… 实际的业务场景中，几十万上百万的句子都算少的，好家伙，这计算量着实有点难顶啊。
can you 顶得住？ 还是让我们回到sentence embedding这个更泛化的问题上来，如果现在有一个NBERT模型，能够得到高质量的句子向量表示，那么面对STS任务，我们就可以先将"sentence 1"作为BERT输入，得到向量"vector 1"，再将"sentence 2"作为BERT输入，得到向量"vector 2"，然后计算两个向量之间的相似度。...</p></div><footer class=entry-footer><span title="2022-07-16 01:55:00 +0000 +0000">July 16, 2022</span>&nbsp;·&nbsp;2 min</footer><a class=entry-link aria-label="post link to Sentence-BERT: 如何通过对比学习得到更好的句子向量表示" href=https://basicv8vc.github.io/posts/sentence-bert/></a></article><article class=post-entry><header class=entry-header><h2>如何申请Google TRC项目，领取免费的Cloud TPU计算资源</h2></header><div class=entry-content><p>背景 当下，深度学习要想搞的好，算力、数据和模型哪一样都少不了。后两者相对来说比较容易解决，各种模型开源实现和公开数据集还是挺充足的，但是算力≈金钱，有时候脑海中蹦出来一些新鲜idea，可是看着手里的老旧显卡，只能在夜晚暗自神伤
额，画风好像有点不对，但是大概就是这么个意思 一种方法是租显卡，比如某A、某B或者某C，如果是个人使用，不必像公司做项目那样顾虑太多，可以挑一些小公司的产品，有的还是挺便宜的。
另一种就是找免费的计算资源了
比如Google Colab或者Kaggle，以及今天要介绍的Google Research的TPU Research Cloud (TRC)项目。
TRC项目 TRC项目中的T，指的是Google自家的加速卡TPU，和GPU不同，Google并不公开出售TPU设备，而是集成在Google Cloud中，提供挂载了TPU的云计算服务。
TRC项目就是Google免费赠送给我们一段时间的TPU服务器，比如这是我申请成功后的结果，5台Cloud TPU v2-8和5台Cloud TPU v3-8，以及100台抢占式的Cloud TPU v2-8。免费使用时间是60天。
随便看一下其中一台的配置，
CPU 96核，内存335GB，而且还挂载了TPU v2-8或者v3-8。我只能说，
Cloud TPU TPU是Google推出的专用于机器学习的加速设备，可以类比NVIDIA的GPU，目前TPU已经更新到了第四代，也就是TPU v4，TRC项目提供的是前两代，TPU v2和TPU v3，对于大部分场景已经绰绰有余了。
简单说一下TPU，每块TPU上面有4块芯片（chip），每块芯片有两个核（core），所以这就是为什么叫做v2-8/v3-8，以v3为例，每个核有2个独立的矩阵计算单元（Matrix Multiply Unit, MXU）、1个向量处理单元（Vector Processing Unit, VPU）以及1个标量单元，每块芯片有32GB的高速存储（HBM）。
所以，可以把v3-8简单理解为就是8张GPU卡。
除此之外，还有算力恐怖的TPU Pod，也就是几百上千块TPU组成的算力单元，当然，这个并没有免费开放给所有人。
TRC申请 由于Cloud TPU是以Google Cloud中的一种服务提供出来，所以先要有一个Google Cloud账号，然后再申请TRC项目。
Google Cloud开通 Google Cloud申请流程，网上有很多资料，这里就不细讲了，必要条件是有一个gmail邮箱、一个手机号以及一张支持VISA或MasterCard的信用卡。
// 貌似这一步是最难的:(
注册成功后，Google Cloud会赠送300$，有效期90天。
TRC申请 如果你开通了Google Cloud，接下来就可以申请TRC项目了，申请流程非常简单，就是填写一个小问卷：
问题很简单，如实填写就行了，貌似就没有被拒的:)...</p></div><footer class=entry-footer><span title="2022-07-07 00:30:29 +0800 CST">July 7, 2022</span>&nbsp;·&nbsp;1 min</footer><a class=entry-link aria-label="post link to 如何申请Google TRC项目，领取免费的Cloud TPU计算资源" href=https://basicv8vc.github.io/posts/trc-program/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://basicv8vc.github.io/page/2/>Next Page »</a></nav></footer></main><footer class=footer><span>&copy; 2022 <a href=https://basicv8vc.github.io/>Yet Another Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>