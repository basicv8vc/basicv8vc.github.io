<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Gradient Boosting 从入门到调包 (入门篇) | Yet Another Blog</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Gradient Boosting 从入门到调包 (入门篇)</h1><a id="logo" href="/.">Yet Another Blog</a><p class="description">Something on Programming.</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Gradient Boosting 从入门到调包 (入门篇)</h1><div class="post-meta">Jul 8, 2016<span> | </span><span class="category"><a href="/categories/math/">math</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-disqus-identifier="2016/07/08/Gradient-Boosting-从入门到调包-入门篇/" href="/2016/07/08/Gradient-Boosting-从入门到调包-入门篇/#disqus_thread" class="disqus-comment-count"></a><div class="post-content"><h1 id="开篇"><a href="#开篇" class="headerlink" title="开篇"></a>开篇</h1><p>虽然数据挖掘算法很多，但有过实际经验的同学肯定都知道，集成方法(ensembel method)大多数情况下都是首选。集成方法和LR、SVM算法不同，她不是一个具体的模型或算法，而是一类算法的统称。集成，从其字面意思也能看出，这家伙不是单兵作战，而是依靠团队取胜。简而言之，集成方法就是通过构建并结合多个学习器来完成学习任务。这句话有两个地方需要注意，“如何构建每一个学习器”以及“如何结合多个学习器而产生预测结果”。</p>
<p>如果我们按照“如何构建每一个学习器”来将集成方法分类，总体可以分为两类：1) 个体学习器减存在强依赖关系、必须串行生成的序列化方法，代表方法是Boosting；2）个体学习器之间不存在强依赖关系、可同时生成的并行化方法，代表方法是Bagging。</p>
<p>本文要讲到的Gradient Boosting就属于Boosting方法。</p>
<p>由于Gradient Boosting涉及到的知识实在是很多，完全可以写成类似”Understanding Random Forest: From Theory to Practice”的博士论文，由于水平和时间的限制，本文不会涉及过多、过深的细节，而是尽量用通俗的自然语言和数学语言来解释我对Gradient Boosting的浅显理解，仅仅是入门水平:)</p>
<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>既然要讲Gradient Boosting，那么决策树肯定是绕不开的，虽然Boosting方法对”个体学习器”没有要求，你可以选择LR、SVM、决策树等等。但是大多数情况下我们默认都选择决策树作为个体学习器。下面简单解释决策树算法。</p>
<p>决策树，顾名思义，根据树结构来进行决策。这种思想是很质朴的，和我们人类做决策时的思考过程很像，比如明天出门要不要带伞啊？如果我根据天气情况来做出决定，可以记作：</p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> 明天下雨：</div><div class="line">    出门带伞</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    出门不带伞</div></pre></td></tr></table></figure>
<p>看到没有，决策过程可以用if, else来具体化，难怪有人说，决策树就是一堆if else而已。数据挖掘比赛中很多人会首先用简单的规则跑出一个结果，这实际上就是在人工构造决策树啊。</p>
<p>既然决策树是一种机器学习方法，我们就想知道她的学习过程是怎么样的，也就是给定训练集如果构建一颗决策树呢？通常，决策树的构建是一个递归过程:</p>
<ul>
<li>1 输入训练集D，特征集F</li>
<li>2 如果D中样本全属于同一个类别C，将当前节点标记为C类叶节点，返回</li>
<li>3 从F中选择最优划分特征f</li>
<li>4 对于f的每一个值$f_i$, 都生成一个子节点，然后对D进行划分，f值相同的样本都放到同一个子节点，并对节点进行标记，然后对于每一个新产生的子节点，进行第1步</li>
</ul>
<p>注意：实际上对于决策树，大多数的实现方法都是基于二叉树，比如sklearn。</p>
<h2 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h2><p>决策树算法的关键是上面的第3步，即如何选择最优划分特征。如何评价一个特征是不是优秀呢？基本的原则就是看按照某一个特征生成子节点后，子节点的样本是不是都尽可能属于同一个类别，也就是子节点的纯度越高，我们就认为这个特征”好”。</p>
<p>为了量化子节点的纯度，前人提出了很多具体的评价方法，包括信息增益、信息增益率和基尼指数。这些评价方法大同小异，我们就以最常见的信息增益(Information Gain)为例进行解释：</p>
<p>假设当前节点包含的数据集为D，则数据集的熵(entropy)定义为：</p>
<p><img src="https://ooo.0o0.ooo/2016/07/09/5780a565a3f03.png" alt=""></p>
<p>其中$|Y|$表示类别值的取值个数。</p>
<p>假设某一个特征$f$有V个可能的取值 $f^{1},f^{2},…,f^{V}$,  若使用特征$f$对$D$进行划分，会产生$V$个子节点，其中第$v$个子节点包含了$D$中所有$f$取值为$f^{v}$, 记作$D^{v}$. 按照下式计算用$f$对$D$进行划分获得的信息增益(information gain):</p>
<p><img src="https://ooo.0o0.ooo/2016/07/09/5780a9c255d9a.png" alt=""></p>
<p>信息增益越大，我们就认为用$f$划分所获得的纯度提升越大。</p>
<p>下面是用信息增益来构建决策树的简单Python代码，注意，构建的是二叉树。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">uniqueCounts</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    对y的取值进行计数, 用于计算节点的purity</div><div class="line">    rvalue: 计数结果</div><div class="line">    """</div><div class="line">    results = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> X:</div><div class="line">        y = X[<span class="number">-1</span>]</div><div class="line">        results[y] = results.get(y, <span class="number">0</span>) + <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> results</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    计算熵</div><div class="line">    """</div><div class="line">    <span class="keyword">from</span> math <span class="keyword">import</span> log</div><div class="line">    log2 = <span class="keyword">lambda</span> x: log(x)/log(<span class="number">2</span>)</div><div class="line">    results = uniqueCounts(X)</div><div class="line"></div><div class="line">    entropy = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> results:</div><div class="line">        p = float(results[y])/len(X)</div><div class="line">        entropy = entropy - p*log2(p)</div><div class="line">    <span class="keyword">return</span> entropy</div><div class="line"></div><div class="line"><span class="comment"># 定义节点</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTreeNode</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, col=<span class="number">-1</span>, value=None, results=None, tb=None, fb=None)</span>:</span></div><div class="line">        self.col = col <span class="comment">#</span></div><div class="line">        self.value = value</div><div class="line">        self.resuls = results</div><div class="line">        self.tb = tb</div><div class="line">        self.fb = fb</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">import</span> numbers</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">divideSet</span><span class="params">(X, column, value)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    用某个特征对数据集进行分割, 分为两个数据集，二叉树</div><div class="line">    """</div><div class="line">    split_function = <span class="keyword">None</span></div><div class="line">    <span class="keyword">if</span> isinstance(value, (numbers.Integral, np.int)) <span class="keyword">or</span> isinstance(value, (numbers.Real, np.float)):</div><div class="line">        split_function = <span class="keyword">lambda</span> row: row[column] &gt;= value</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        split_function = <span class="keyword">lambda</span> row: row[column] == value</div><div class="line"></div><div class="line">    <span class="comment"># 将数据集分为两个集合，并返回</span></div><div class="line">    set1 = [row <span class="keyword">for</span> row <span class="keyword">in</span> X <span class="keyword">if</span> split_function(row)]</div><div class="line">    set2 = [row <span class="keyword">for</span> row <span class="keyword">in</span> X <span class="keyword">if</span> <span class="keyword">not</span> split_function(row)]</div><div class="line">    <span class="keyword">return</span> (set1, set2)</div><div class="line"></div><div class="line">    <span class="comment">#</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildTree</span><span class="params">(X, scoref=entropy)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    以递归方式构建树， 使用信息增益</div><div class="line">    """</div><div class="line">    <span class="keyword">if</span> len(X)==<span class="number">0</span>:</div><div class="line">        <span class="keyword">return</span> DecisionTreeNode()</div><div class="line"></div><div class="line">    current_score = scoref(X)</div><div class="line">    <span class="comment"># 记录最佳拆分属性, 以及用哪个属性值</span></div><div class="line">    best_gain = <span class="number">0.0</span></div><div class="line">    best_criteria = <span class="keyword">None</span> <span class="comment"># 拆分用的属性和属性取值</span></div><div class="line">    best_sets = <span class="keyword">None</span> <span class="comment"># 拆分后的两个数据子集</span></div><div class="line"></div><div class="line">    feature_nums = X.shape[<span class="number">1</span>] - <span class="number">1</span></div><div class="line">    <span class="comment"># 遍历每一个特征以及每个特征的每一个取值来构建树，复杂度很高, 可以用特征的中位数</span></div><div class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> range(feature_nums):</div><div class="line">    <span class="comment"># 在当前列中生成一个由不同值构成的序列</span></div><div class="line">        column_values = &#123;&#125; <span class="comment">#记录数据集中第col个特征，每个不同取值的个数</span></div><div class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> X:</div><div class="line">            column_values[row[col]] = <span class="number">1</span> <span class="comment"># 初始化</span></div><div class="line"></div><div class="line">        <span class="comment"># 根据当前列的特征每个取值， 尝试对数据集进行分割</span></div><div class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> column_values:</div><div class="line">            set1, set2 = divideSet(X, col, value)</div><div class="line"></div><div class="line">            <span class="comment"># 信息增益</span></div><div class="line">            p = float(len(set1)) / len(X)</div><div class="line">            gain = current_score - p*scoref(set1) - (<span class="number">1</span>-p)*scoref(set2)</div><div class="line">            <span class="keyword">if</span> gain &gt; best_gain <span class="keyword">and</span> len(set1) &gt;<span class="number">0</span> <span class="keyword">and</span> len(set2) &gt;<span class="number">0</span> :</div><div class="line">                best_gain = gain</div><div class="line">                best_criteria = (col, value)</div><div class="line">                best_sets = (set1, set2)</div><div class="line"></div><div class="line">    <span class="comment"># 创建子分之</span></div><div class="line">    <span class="keyword">if</span> best_gain &gt; <span class="number">0</span>:</div><div class="line">        trueBranch = buildTree(best_sets[<span class="number">0</span>])</div><div class="line">        falseBranch = buildTree(best_sets[<span class="number">1</span>])</div><div class="line">        <span class="keyword">return</span> DecisionTreeNode(col=best_criteria[<span class="number">0</span>], value=best_criteria[<span class="number">1</span>],</div><div class="line">                                    tb = trueBranch, fb=falseBranch)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> DecisionTreeNode(results=uniqueCounts(X))</div></pre></td></tr></table></figure></p>
<h1 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h1><p>经常参加数据挖掘比赛(Kaggle、天池)的同学肯定对XGBoost、GBDT如雷贯耳，特别是XGBoost，简直堪称DM居家必备之神器。马克思老人家说过，透过现象看本质。XGBoost之所以这么吊，除了牛叉的代码实现，也和她的算法原理, Gradient Boosting, 有直接联系。下面我们就开启Gradient Boosting入门之旅吧。</p>
<p>Gradient Boosting, 又被称为Gradient Boosting Decision Tree(GBDT)、Gradient Boosting Machines(GBMs) 或者 Gradient Boosted Trees(GBT), 虽然别名很多，但只要看到<strong>gradient</strong>和<strong>boost</strong>这两个词，就能确定指的就是本文的Gradient Boosting.</p>
<p>Gradient Boosting除了名字多以外，她的应用范围也很广，可直接应用于分类(Classification)、回归(Regression)和排序(Ranking)。</p>
<h2 id="残差和梯度"><a href="#残差和梯度" class="headerlink" title="残差和梯度"></a>残差和梯度</h2><p>和其他Boosting算法一样，Gradient Boosting可以记作：</p>
<p><img src="https://ooo.0o0.ooo/2016/07/09/5780fc2fd9140.png" alt=""></p>
<p>这个式子很好理解，$h_{t}(x)$是个体学习器，Gradient Boosting就是把每一个个体学习器集成到一起。</p>
<p>前面我们提到Boosting通过串行的方式构建个体学习器，换句话说，个体学习器是按照顺序一个个构建的，比如迭代式或者递归的方式。而每一个学习器当然也不是随意创建的，比如AdaBoost每一个学习器都会重点关注上一个学习器分类错误的那些样本点，通过为每个样本赋予不同的权重实现。</p>
<p>我们先不讲Gradient Boosting背后的原理，单就看他的名字，我当初就很疑惑，<strong>gradient</strong>? 模型集成和梯度有半毛钱关系啊？更别提GBDT里面不但有<strong>gradient</strong>还有<strong>tree</strong>，梯度和决策树是怎么搞到一起的？</p>
<p>带着这些疑问，让我们一步步揭开Gradient Boosting的面纱。</p>
<p>首先，来思考一个回归问题：现有训练集D={$x_i,y_i$}和一个已经在D上训练好的模型$F$, 不过$F$在D上的表现不太好，你能否按照如下规则来得到一个令人满意的模型：</p>
<ul>
<li>不能对$F$做改动</li>
<li>你可以利用D单独训练决策树模型h，但是必须用F(x)+h(x)作为最终的模型</li>
</ul>
<p>我们希望最后的模型F+h效果如下：</p>
<p><img src="https://ooo.0o0.ooo/2016/07/09/5781072c047d3.png" alt=""></p>
<p>上面的等式等价为：</p>
<p><img src="https://ooo.0o0.ooo/2016/07/09/578108a209f45.png" alt=""></p>
<p>看到这里你会发现，对于决策树h，她的训练集实际上是{$x_i$, $y_i-F(x_i)$}！而y-F(x)在数学界是有名号的，被称作残差(residual)。 我们再回过头来看h的物理意义，她的作用其实是修正F的错误，也就是抵消F的误差。</p>
<p>如果F+h在D上的表现还是令人不太满意。</p>
<p><img src="https://ooo.0o0.ooo/2016/07/09/57810eadb2ae4.png" alt=""></p>
<p>我们不断构建决策树，最终会得到一个强大的模型F+$\sumh_{t}$, 这实际上就是Gradient Boosting的学习过程。</p>
<p><img src="https://ooo.0o0.ooo/2016/07/09/5781183f08ba3.png" alt=""></p>
<p>看到这里你肯定会疑惑，这明明是 Residual Boosting！和梯度有毛线关系啊？ 且看下面的推导：</p>
<p>假设我们要训练模型F(x), 损失函数选用平方差损失函数,</p>
<p><img src="https://ooo.0o0.ooo/2016/07/09/5781198fc70cb.png" alt=""></p>
<p>我们想要最小化:</p>
<p><img src="https://ooo.0o0.ooo/2016/07/09/57811a1d4e830.png" alt=""></p>
<p>由于F($x<em>{i}$)也是一个实数，我们不妨把F($x</em>{i}$)当做L的参数，如果用梯度下降算法来优化L,</p>
<p><img src="https://ooo.0o0.ooo/2016/07/09/57811a921a78e.png" alt=""></p>
<p>仔细看梯度，竟然和残差有直接的联系：</p>
<p><img src="https://ooo.0o0.ooo/2016/07/09/57811ad08b752.png" alt=""></p>
<p><img src="https://ooo.0o0.ooo/2016/07/09/57811b5a8b413.png" alt=""></p>
<p>现在，我们可以用梯度下降的思想来解释损失函数为平方差时，Gradient Boosting的学习过程：</p>
<ul>
<li>1 构建初始学习器F, 比如$y_{i}$的平均值</li>
<li>2 计算平方差损失函数的梯度g</li>
<li>3 用(x,-g) 训练决策树h</li>
<li>4 F = F + 1*h</li>
<li>5 如果决策树个数达到阈值，结束；否则，跳转到第2步</li>
</ul>
<p>Gradient Boosting可不是仅能用平方差损失函数哦，以sklearn为例，用于回归的GradientBoostingRegressor支持least square loss、least absolute loss、hubor loss和quantile loss；用于分类的GradientBoostingClassificier支持logistic regression和exponential loss。</p>
<p>我们可以把梯度看做残差的泛化形式，则任何函数都可以作为Gradient Boosting的损失函数！只要你能提供她的梯度！这也是XGBoost支持自定义损失函数的原理基础。</p>
<p>所以，Gradient Boosting的学习过程：</p>
<ul>
<li>1 构建初始学习器F, 比如$y_{i}$的平均值</li>
<li>2 计算损失函数的梯度g</li>
<li>3 用(x,-g) 训练决策树h</li>
<li>4 F = F + $\rho$ h, $\rho$为学习率</li>
<li>5 如果决策树个数达到阈值，结束；否则，跳转到第2步</li>
</ul>
<h2 id="过拟合与正则化"><a href="#过拟合与正则化" class="headerlink" title="过拟合与正则化"></a>过拟合与正则化</h2><p>随着Gradient Boosting中决策树个数的增加，通常你会发现在训练集D上的效果越来越好，如果是二分类问题，甚至可能准确率接近100%，此时就要注意了，很可能产生了过拟合(overfitting)现象。任何一个能实际应用的模型，都必然会考虑到过拟合现象，我们来看一下Gradient Boosting如何减弱过拟合。</p>
<h3 id="子采样-subsampling"><a href="#子采样-subsampling" class="headerlink" title="子采样(subsampling)"></a>子采样(subsampling)</h3><p>一个模型如果在训练集上表现很好，在测试集上表现远不如训练集，我们就认为这个模型陷入了过拟合。她把训练集中存在的噪声错误地归类为数据的普遍性规律，怎么样降低噪声对模型的干扰呢？如果我们把噪声看做数据集中的异常点，在训练模型的时候，我们引入随机性因素，比如从训练集中随机抽取样本来训练模型，无疑会减弱异常点的干扰。这就是子采样的思想。</p>
<p>在每一次构建新的决策树之前，先从训练集中随机抽取部分样本构建一个新的训练集$D_t$，然后利用$D_t$训练决策树。</p>
<p>sklearn的GradientBoostingRegressor和GradientBoostingClassifier提供了<em>subsample</em>参数进行子采样。</p>
<h2 id="收缩-shrinkage"><a href="#收缩-shrinkage" class="headerlink" title="收缩(shrinkage)"></a>收缩(shrinkage)</h2><p>Shrinkage也是常用的减弱过拟合的手段之一，在Gradient Boosting中，shrinkage用于降低每一个个体学习器的影响，在sklearn中，shrinkage就是我们通常见到的学习率, 即，GradientBoostingRegressor和GradientBoostingClassifier中的<em>learning_rate</em>参数。</p>
<p>这里多说一句，原始的Gradient Boosting算法在每一次学习到个体学习器后，还要学习一个参数叫做步长(step length)的参数，步长和shrinkage是两个参数，不要搞混，但是在sklearn的实现中，并没有考虑步长，直接用学习率代替了shrinkage.</p>
<p><img src="https://ooo.0o0.ooo/2016/07/10/5781dd36047cc.png" alt=""></p>
<p>用来减弱过拟合的方法有很多，比如还有early stopping, GradientBoostingRegressor和GradientBoostingClassifier还提供了$monitor$参数, 可以用于early stopping。</p>
<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>Gradient Boosting的开源实现有很多，但目前最流行最好用的肯定是XGBoost。 XGBoost之所以这么好用，一部分原因是陈天奇等人强大的编码能力和优秀的系统设计，另一方面也是因为她对传统Gradient Boosting在算法层面做出了诸多改进。本文不涉及并行化和系统设计的内容， 我仅谈一下XGBoost在算法层面的改进之处。</p>
<h3 id="带有正则项的目标函数"><a href="#带有正则项的目标函数" class="headerlink" title="带有正则项的目标函数"></a>带有正则项的目标函数</h3><p>大部分机器学习算法的学习过程可以转换成一个优化问题，目标函数=损失函数＋正则项。而传统的Gradient Boosting是不包含正则项的，所以，XGBoost做出的第一项改进就是修改目标函数，增加如下的正则项：</p>
<p><img src="https://ooo.0o0.ooo/2016/07/10/5782491de52e2.png" alt=""></p>
<p>这个正则项量化了每一颗CART树的复杂度， 其中T是CART树$f_t$的叶子节点数目，$w$是叶子节点的权重。 看一个简单的例子：</p>
<p><img src="https://ooo.0o0.ooo/2016/07/10/57824a2f82ac7.png" alt=""></p>
<h3 id="目标函数的近似"><a href="#目标函数的近似" class="headerlink" title="目标函数的近似"></a>目标函数的近似</h3><p>XGBoost在对目标函数进行优化求解时，采用了近似手段，</p>
<p><img src="https://ooo.0o0.ooo/2016/07/10/57824d4276cb1.png" alt=""></p>
<p>然后对损失函数 泰勒展开：</p>
<p><img src="https://ooo.0o0.ooo/2016/07/10/57824e6e26218.png" alt=""></p>
<p><img src="https://ooo.0o0.ooo/2016/07/10/57824eb02477f.png" alt=""></p>
<p>进一步移除目标函数中的常量，</p>
<p><img src="https://ooo.0o0.ooo/2016/07/10/57824f3dceb90.png" alt=""></p>
<p>上面的$f_{t}(x)$表示一颗决策树，我们能否只用叶子节点表示一棵树呢？答案是可以，但需要知道两方面的知识，一是叶子节点的权重$w$, 二是每个叶子节点的样本点(即，树的结构)，用p表示。</p>
<p><img src="https://ooo.0o0.ooo/2016/07/10/57825114a0d75.png" alt=""></p>
<p>进一步我们按照样本点在CART树种的分布情况，可以将训练集样本分为T组，得到：</p>
<p><img src="https://ooo.0o0.ooo/2016/07/10/578255aed739c.png" alt=""></p>
<p><img src="https://ooo.0o0.ooo/2016/07/10/578255c63875d.png" alt=""></p>
<p>上式其实是T个二次函数的和，我们知道对于单变量二次函数$ax^2+bx$，当x取值为$-b/(2a)$时，函数有极值$-b^2/(4a)$,</p>
<p><img src="https://ooo.0o0.ooo/2016/07/10/57825965551ed.png" alt=""></p>
<p>我们可以得到每个叶子节点对应的$w$的最佳取值和函数极值:</p>
<p><img src="https://ooo.0o0.ooo/2016/07/10/578259e231824.png" alt=""></p>
<p>看一个具体的例子：</p>
<p><img src="https://ooo.0o0.ooo/2016/07/10/57825a4cf1231.png" alt=""></p>
<h3 id="近似贪心算法"><a href="#近似贪心算法" class="headerlink" title="近似贪心算法"></a>近似贪心算法</h3><p>决策树分割节点时，一般使用贪心算法，并且需要遍历所有特征以及特征的每一个取值，这种做法称为精确贪心算法(exact greedy algorithm), XGBoost和sklearn一样支持这种精确贪心算法，除此之外，XGBoost还包含了陈天奇等人提出的决策树分割节点的近似算法和 针对稀疏数据的分割节点快速算法。对于近似算法，我说一下自己的错误理解:</p>
<p>不论精确还是近似算法，都需要遍历一遍特征，近似算法的有点是不需要遍历特征的每个取值，她只遍历特征取值范围的百分位数上那些取值，那么如何取得这些百分位数值呢？可以用quantile sketch算法，事情到这里是不是就结束了呢？No，陈提到quantile sketch算法没有考虑到样本权重问题，如果数据集中的数据点是有权重的，这个算法就不适用了。于是陈等人提出了一种可以使用于带权重数据的quantile sketch算法。</p>
<h3 id="其他优点"><a href="#其他优点" class="headerlink" title="其他优点"></a>其他优点</h3><p>前面说过，理论上Gradient Boosting的损失函数可以是任意可微函数。只要能够提供损失函数的梯度和Hessian矩阵，用户就可以自定义训练模型时的损失函数。 XGBoost也允许用户在交叉验证时自定义误差衡量标准。 也可以选择使用线性模型替代树模型，从而得到带L1+L2惩罚的线性回归或者logistic回归。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://ljblog.org/2016/07/08/Gradient-Boosting-从入门到调包-入门篇/" data-id="ciwgimzgd0006d8mzu4gzaw90" class="article-share-link">分享到</a><div class="tags"><a href="/tags/机器学习/">机器学习</a><a href="/tags/Gradient-Boosting/">Gradient Boosting</a></div><div class="post-nav"><a href="/2016/07/08/Gradient-Boosting-从入门到调包-调包篇/" class="pre">Gradient Boosting 从入门到调包 (调包篇)</a><a href="/2015/09/14/Bayesian-Nets-Graph-Model-画图工具/" class="next">Bayesian Nets/Graph Model 画图工具</a></div><div id="disqus_thread"><script>var disqus_shortname = 'ljblog-org';
var disqus_identifier = '2016/07/08/Gradient-Boosting-从入门到调包-入门篇/';
var disqus_title = 'Gradient Boosting 从入门到调包 (入门篇)';
var disqus_url = 'http://ljblog.org/2016/07/08/Gradient-Boosting-从入门到调包-入门篇/';
(function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//ljblog-org.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://ljblog.org"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/Gradient-Boosting/" style="font-size: 15px;">Gradient Boosting</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/LaTeX/" style="font-size: 15px;">LaTeX</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/聚类/" style="font-size: 15px;">聚类</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/12/08/使用pdb调试Pyton代码/">使用pdb调试Pyton代码</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/04/Java内存模型/">Java内存模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/07/19/K-Means从入门到调包/">K-Means从入门到调包</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/07/08/Gradient-Boosting-从入门到调包-调包篇/">Gradient Boosting 从入门到调包 (调包篇)</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/07/08/Gradient-Boosting-从入门到调包-入门篇/">Gradient Boosting 从入门到调包 (入门篇)</a></li><li class="post-list-item"><a class="post-list-link" href="/2015/09/14/Bayesian-Nets-Graph-Model-画图工具/">Bayesian Nets/Graph Model 画图工具</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.weibo.com/basicvbvc/" title="微博" target="_blank">微博</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Yet Another Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-39855248-3','auto');ga('send','pageview');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>