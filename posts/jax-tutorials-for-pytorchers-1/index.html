<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>面向PyTorch用户的JAX简易教程[1]: JAX介绍 | Yet Another Blog</title><meta name=keywords content="JAX,TPU,机器学习"><meta name=description content="背景 前几天申请参加了Google TRC项目，TPU VM的配置相当可以，但是PyTorch/XLA做数据并行时的体验却并不那么丝滑，考虑到Google一直力推TPU+JAX的组合，所以决定学习下JAX。
JAX简介 什么是JAX？
官方在GitHub README中是这么介绍的: JAX is Autograd and XLA, brought together for high-performance machine learning research.
在Description中写的是: Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more.
在JAX官方文档又是这么介绍的: JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.
总结一下，有几个关键词：Autograd、XLA、NumPy和composable transformations。
XLA 先来说XLA，这个我了解的最少，所以介绍起来最简单，XLA (Accelerated Linear Algebra)是Google为TensorFlow设计的一款编译器，主打JIT (Just-in-Time)编译和跨设备（CPU/GPU/TPU）执行，所以JAX介绍中凡是涉及到JIT、high-performance、CPU/GPU/TPU，都指的是XLA。
NumPy NumPy就不用提了，Python生态下只要涉及到数据分析/机器学习/数值计算中对数组/tensor进行操作，都离不开它，不夸张的说，NumPy API已经成为了数组/tensor操作的半个工业标准，包括各家深度学习框架中对tensor操作的函数接口也都是尽量靠近NumPy，JAX则更夸张，jax.numpy重新实现一套了NumPy API ，让用户从NumPy无缝切入JAX：
from jax import numpy as jnp Autograd 这里的Autograd是哈佛大学HIPS实验室在14年开始开发的一款自动微分框架，特点是可以对Python/NumPy函数进行高阶求导，直接看个例子，一个简单的函数 f(x) ，顺便求一下一阶、二阶、三阶导函数："><meta name=author content><link rel=canonical href=https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-1/><link crossorigin=anonymous href=/assets/css/stylesheet.min.ed52a04cba0843fef8297e018b15e8a32a989ea4415133cb8bf77414d3815f7b.css integrity="sha256-7VKgTLoIQ/74KX4BixXooyqYnqRBUTPLi/d0FNOBX3s=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://basicv8vc.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://basicv8vc.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://basicv8vc.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://basicv8vc.github.io/apple-touch-icon.png><link rel=mask-icon href=https://basicv8vc.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:title" content="面向PyTorch用户的JAX简易教程[1]: JAX介绍"><meta property="og:description" content="背景 前几天申请参加了Google TRC项目，TPU VM的配置相当可以，但是PyTorch/XLA做数据并行时的体验却并不那么丝滑，考虑到Google一直力推TPU+JAX的组合，所以决定学习下JAX。
JAX简介 什么是JAX？
官方在GitHub README中是这么介绍的: JAX is Autograd and XLA, brought together for high-performance machine learning research.
在Description中写的是: Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more.
在JAX官方文档又是这么介绍的: JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.
总结一下，有几个关键词：Autograd、XLA、NumPy和composable transformations。
XLA 先来说XLA，这个我了解的最少，所以介绍起来最简单，XLA (Accelerated Linear Algebra)是Google为TensorFlow设计的一款编译器，主打JIT (Just-in-Time)编译和跨设备（CPU/GPU/TPU）执行，所以JAX介绍中凡是涉及到JIT、high-performance、CPU/GPU/TPU，都指的是XLA。
NumPy NumPy就不用提了，Python生态下只要涉及到数据分析/机器学习/数值计算中对数组/tensor进行操作，都离不开它，不夸张的说，NumPy API已经成为了数组/tensor操作的半个工业标准，包括各家深度学习框架中对tensor操作的函数接口也都是尽量靠近NumPy，JAX则更夸张，jax.numpy重新实现一套了NumPy API ，让用户从NumPy无缝切入JAX：
from jax import numpy as jnp Autograd 这里的Autograd是哈佛大学HIPS实验室在14年开始开发的一款自动微分框架，特点是可以对Python/NumPy函数进行高阶求导，直接看个例子，一个简单的函数 f(x) ，顺便求一下一阶、二阶、三阶导函数："><meta property="og:type" content="article"><meta property="og:url" content="https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-1/"><meta property="og:image" content="https://basicv8vc.github.io/%3Cimage%20path/url%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-07-21T01:31:00+00:00"><meta property="article:modified_time" content="2022-07-21T01:31:00+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://basicv8vc.github.io/%3Cimage%20path/url%3E"><meta name=twitter:title content="面向PyTorch用户的JAX简易教程[1]: JAX介绍"><meta name=twitter:description content="背景 前几天申请参加了Google TRC项目，TPU VM的配置相当可以，但是PyTorch/XLA做数据并行时的体验却并不那么丝滑，考虑到Google一直力推TPU+JAX的组合，所以决定学习下JAX。
JAX简介 什么是JAX？
官方在GitHub README中是这么介绍的: JAX is Autograd and XLA, brought together for high-performance machine learning research.
在Description中写的是: Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more.
在JAX官方文档又是这么介绍的: JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.
总结一下，有几个关键词：Autograd、XLA、NumPy和composable transformations。
XLA 先来说XLA，这个我了解的最少，所以介绍起来最简单，XLA (Accelerated Linear Algebra)是Google为TensorFlow设计的一款编译器，主打JIT (Just-in-Time)编译和跨设备（CPU/GPU/TPU）执行，所以JAX介绍中凡是涉及到JIT、high-performance、CPU/GPU/TPU，都指的是XLA。
NumPy NumPy就不用提了，Python生态下只要涉及到数据分析/机器学习/数值计算中对数组/tensor进行操作，都离不开它，不夸张的说，NumPy API已经成为了数组/tensor操作的半个工业标准，包括各家深度学习框架中对tensor操作的函数接口也都是尽量靠近NumPy，JAX则更夸张，jax.numpy重新实现一套了NumPy API ，让用户从NumPy无缝切入JAX：
from jax import numpy as jnp Autograd 这里的Autograd是哈佛大学HIPS实验室在14年开始开发的一款自动微分框架，特点是可以对Python/NumPy函数进行高阶求导，直接看个例子，一个简单的函数 f(x) ，顺便求一下一阶、二阶、三阶导函数："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://basicv8vc.github.io/posts/"},{"@type":"ListItem","position":2,"name":"面向PyTorch用户的JAX简易教程[1]: JAX介绍","item":"https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"面向PyTorch用户的JAX简易教程[1]: JAX介绍","name":"面向PyTorch用户的JAX简易教程[1]: JAX介绍","description":"背景 前几天申请参加了Google TRC项目，TPU VM的配置相当可以，但是PyTorch/XLA做数据并行时的体验却并不那么丝滑，考虑到Google一直力推TPU+JAX的组合，所以决定学习下JAX。\nJAX简介 什么是JAX？\n官方在GitHub README中是这么介绍的: JAX is Autograd and XLA, brought together for high-performance machine learning research.\n在Description中写的是: Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more.\n在JAX官方文档又是这么介绍的: JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n总结一下，有几个关键词：Autograd、XLA、NumPy和composable transformations。\nXLA 先来说XLA，这个我了解的最少，所以介绍起来最简单，XLA (Accelerated Linear Algebra)是Google为TensorFlow设计的一款编译器，主打JIT (Just-in-Time)编译和跨设备（CPU/GPU/TPU）执行，所以JAX介绍中凡是涉及到JIT、high-performance、CPU/GPU/TPU，都指的是XLA。\nNumPy NumPy就不用提了，Python生态下只要涉及到数据分析/机器学习/数值计算中对数组/tensor进行操作，都离不开它，不夸张的说，NumPy API已经成为了数组/tensor操作的半个工业标准，包括各家深度学习框架中对tensor操作的函数接口也都是尽量靠近NumPy，JAX则更夸张，jax.numpy重新实现一套了NumPy API ，让用户从NumPy无缝切入JAX：\nfrom jax import numpy as jnp Autograd 这里的Autograd是哈佛大学HIPS实验室在14年开始开发的一款自动微分框架，特点是可以对Python/NumPy函数进行高阶求导，直接看个例子，一个简单的函数 f(x) ，顺便求一下一阶、二阶、三阶导函数：","keywords":["JAX","TPU","机器学习"],"articleBody":"背景 前几天申请参加了Google TRC项目，TPU VM的配置相当可以，但是PyTorch/XLA做数据并行时的体验却并不那么丝滑，考虑到Google一直力推TPU+JAX的组合，所以决定学习下JAX。\nJAX简介 什么是JAX？\n官方在GitHub README中是这么介绍的: JAX is Autograd and XLA, brought together for high-performance machine learning research.\n在Description中写的是: Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more.\n在JAX官方文档又是这么介绍的: JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n总结一下，有几个关键词：Autograd、XLA、NumPy和composable transformations。\nXLA 先来说XLA，这个我了解的最少，所以介绍起来最简单，XLA (Accelerated Linear Algebra)是Google为TensorFlow设计的一款编译器，主打JIT (Just-in-Time)编译和跨设备（CPU/GPU/TPU）执行，所以JAX介绍中凡是涉及到JIT、high-performance、CPU/GPU/TPU，都指的是XLA。\nNumPy NumPy就不用提了，Python生态下只要涉及到数据分析/机器学习/数值计算中对数组/tensor进行操作，都离不开它，不夸张的说，NumPy API已经成为了数组/tensor操作的半个工业标准，包括各家深度学习框架中对tensor操作的函数接口也都是尽量靠近NumPy，JAX则更夸张，jax.numpy重新实现一套了NumPy API ，让用户从NumPy无缝切入JAX：\nfrom jax import numpy as jnp Autograd 这里的Autograd是哈佛大学HIPS实验室在14年开始开发的一款自动微分框架，特点是可以对Python/NumPy函数进行高阶求导，直接看个例子，一个简单的函数 f(x) ，顺便求一下一阶、二阶、三阶导函数：\n\\[f(x) = x^3 + 2 \\cdot x \\]\n\\[f’(x) = 3 \\cdot x^2 + 2 \\]\n\\[f’’(x) = 6 \\cdot x \\]\n\\[f’’’(x) = 6 \\]\n如果\\(x = 2 \\) ，甚至可以口算出 \\(f’(2) = 14, f’’(2)=12, f’’’(2)=6 \\) 。我们可以用autograd来实现求导：\nfrom autograd import grad  def f(x):  return x**3 + 2*x  grad_f = grad(f) # 一阶导函数 grad_grad_f = grad(grad_f) # 两次grad组合，就是二阶导函数 grad_grad_grad_f = grad(grad_grad_f) # 三次grad组合，就是三阶导函数 print(grad_f(2.), grad_grad_f(2.), grad_grad_grad_f(2.)) # 14.0 12.0 6.0 自动微分框架除了可以应用于数值计算，它还是深度学习框架的核心，可惜的是，由于性能（纯Python，只有CPU版本）以及其他原因，autograd库并没有推广起来，但是它却实实在在启发到了后续的torch-autograd、Chainer以及PyTorch中的autograd模块：\n 注：Adam毕业后加入了JAX团队，PyTorch在1.10版本也推出了functorch (JAX-like composable function transforms for PyTorch), 他们都有光明的未来:)\n估计是Matthew一直对autograd性能耿耿于怀，当他在Google内部听到XLA的分享后，便和同事产生了JAX的最初想法：\nAutograd + XLA === JAX\n前者负责微分功能，后者实现高性能。\n注：可以将JAX中的算子(operation，操作)看做是对XLA算子的Python封装：jax.numpy中的操作/算子是对更底层的jax.lax的封装，而jax.lax中的算子是XLA算子的Python封装。\nComposable (function) transformations (可组合的函数转换) composable transformations是JAX的核心，真正体现了JAX的特性/差异/优势。 // 标题都改成一级标题了。\n什么是transformation (function transformations, transforms)？其实就是高阶函数 (Higher-order function)，高阶函数是至少满足下列一个条件的函数：\n 接受一个或多个函数作为输入 输出一个函数  Python中常见的高阶函数比如map：\n transformation的输入是Python函数，输出也是函数。JAX中经常用到的transformation主要有四个：\n grad: reverse mode自动微分，用在深度学习中足够了 jit: JIT编译，调用XLA进行JIT编译，用于优化代码 vmap: vectorization/batching，将函数扩展为支持批处理 pmap: parallelization，轻松实现数据并行 (data parallelism)，类似PyTorch的DistributedDataParallel  不知道看到这里，你是不是会很疑惑，JAX的核心就是这么几个高阶函数？能干啥？\n 我们来看下这四个transformation到底能干啥？\ngrad grad在Autograd那里已经介绍过了，\nfrom jax import numpy as jnp from jax import grad  def f(x):  return jnp.sum(x * x) # 函数输出只能是标量  grad_f = grad(f) grad_f(jnp.array([1, 2, 3.])) # DeviceArray([2., 4., 6.], dtype=float32) grad只是JAX自动微分机制中最基本的一个transform，实际上JAX支持前向(forward-mode)和后向(reverse-mode)自动微分以及二者的任意组合， 感兴趣的同学可以去查看jvp和vjp 的文档。考虑到常见的深度学习任务，grad戳戳有余， 其他transform这里就不介绍了，实际上是我没用过，压根没那个能力介绍。\ngrad不但好用，而且数学上更直观，如果我们不局限在深度学习领域，从优化 (optimization)的角度看，大多数机器学习模型的学习都可以表示为\\(\\hat{y} =f(x)\\) 、\\(\\max_{{y}}\\ p(y|x)\\) 、\\(\\max_{y} \\ \\frac{p(x, y)}{p(x)}\\) 的一种。\nLR可以表示为\\(f(x)\\)，神经网络也可以表示为\\(f(x)\\)，损失函数是\\(loss = g(f(x), y)\\)，如果用SGD算法来解决，需要计算参数的梯度，想一下高数课上我们是怎么做的，直接对损失函数求导函数\\(grad(g)\\) 然后代入\\(x\\) ，现在grad用的就是这种方式。并且这种方式在数学上可以自然的泛化到高阶导数优化求解问题上。\njit jit 是用户显式的调用XLA对代码进行优化（包括算子融合、内存优化等），执行时间可能缩短很多，\n import numpy as np from jax import numpy as jnp from jax import jit  def norm(X):  X = X - X.mean(0)  return X / X.std(0)  norm_compiled = jit(norm) X = jnp.array(np.random.rand(10000, 100))  %timeit norm(X).block_until_ready() %timeit norm_compiled(X).block_until_ready() # 585 µs ± 85.4 µs per loop (mean ± std. dev. of 7 runs, 1 loop each) # 216 µs ± 12.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)  # 好像提升不是很显著，再来看一个例子 from jax import random  key = random.PRNGKey(0)  def selu(x, alpha=1.67, lmbda=1.05):  return lmbda * jnp.where(x  0, x, alpha * jnp.exp(x) - alpha)  selu_jit = jit(selu) x = random.normal(key, (1000000,)) %timeit selu(x).block_until_ready() %timeit selu_jit(x).block_until_ready() # 1.06 ms ± 26.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) # 187 µs ± 19.6 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)  # 哦豁，效果还不错 vmap vmap可以自动让函数支持batching，看个例子，原始函数表示的是向量-向量乘法，使用vmap可以得到矩阵-向量乘法的函数：\nfrom jax import numpy as jnp from jax import vmap   def vec_vec_dot(x, y):  \"\"\"vector-vector dot, ([a], [a]) - [] \"\"\"  return jnp.dot(x, y)  x = jnp.array([1,1,2]) y = jnp.array([2,1,1,]) vec_vec_dot(x, y) # DeviceArray(5, dtype=int32)  mat_vec = vmap(vec_vec_dot, in_axes=(0, None), out_axes=0) # ([b,a], [a]) - [b] (b is the mapped axis) xx = jnp.array([[1,1,2], [1,1,2]]) mat_vec(xx, y) # DeviceArray([5, 5], dtype=int32) 解释下vmap中的in_axes和out_axees两个参数，前者表示对输入参数中哪一个的哪一维度进行batch扩充，这里(0, None)表示对x的第0维扩充，由原来的[a] - [b,a]。后者表示对返回结果的哪一维度进行扩充，这里表示由原来的[] -  [b]。\npmap pmap让并行编程变的非常丝滑，可以用于数据并行训练，注意pmap包含了jit操作，下面我就在TPU v3-8 VM演示下：\nimport jax from jax import numpy as jnp from jax import pmap  jax.device_count() # 8个core # 8  jax.devices() \"\"\" [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] \"\"\"  x = jnp.arange(8) y = jnp.arange(8)  vmap(jnp.add)(x, y) # DeviceArray([ 0, 2, 4, 6, 8, 10, 12, 14], dtype=int32)  pmap(jnp.add)(x, y) # ShardedDeviceArray([ 0, 2, 4, 6, 8, 10, 12, 14], dtype=int32) 看到上面vmap和pmap执行后的区别没，一个返回数据类型是DeviceArray，一个则是SharedDeviceArray，后者表示数据分散在多个device中。\n组合 上面介绍的transformation不仅仅可以单兵作战，最重要的是可以任意组合，比如\npmap(vamp(some_func)) jit(grad(grad(vmap(some_func)))) 纯函数约束 transformation很好用，但是只能作用于纯函数 (pure function)。\n或者反过来理解，正因为函数都是纯函数，才可以实现composable transformations这样灵活强大的功能。\n什么是纯函数？\n 只要函数的传参不变，函数返回结果就要相同 函数不会改变函数外的状态  我们直接来看反例吧，第一个反例：\n x = 3.  def not_pure_function_case1(a):  return x + a  print(not_pure_function_case1(1.)) # 4.  x = 5. print(not_pure_function_case1(1.)) # 6. 我们使用相同的传参(1.)调用了两次，可是函数结果不同，所以违背了第一条原则。这是因为函数内部使用了全局变量，虽然仅仅是read value，但是只要全局变量的值改变，函数返回结果就变了。\nTip: 纯函数内部不要读或写函数外的变量。\n第二个反例：\nimport numpy as np  np.random.seed(123) # 设置随机数种子  def not_pure_function_case2(n):  return np.random.randn(n)  not_pure_function_case2(5) # array([-1.0856306 , 0.99734545, 0.2829785 , -1.50629471, -0.57860025])  not_pure_function_case2(5) # array([ 1.65143654, -2.42667924, -0.42891263, 1.26593626, -0.8667404 ]) 随机数在机器学习中太常见了，你看，为了结果可复现，我们还设置了随机数种子，但是，这却不是一个纯函数。\n在NumPy中，随机数生成器状态(RNG State)是一个全局变量，只要我们调用了随机数生成算法（比如上面的np.random.rand()），都会导致RNG State发生变化，这样，连续两次的随机数生成结果就不相同，又违背了纯函数第一条原则。\n为此，jax.numpy和numpy的第一个不同之处出现了，JAX没有隐含的全局RNG State，凡是涉及到随机数生成的地方，都需要用户显式的使用RNG State。\nimport jax  key = jax.random.PRNGKey(0) # 显式的创建PRNGKey，可以表示RNG State  x = jax.random.normal(key, (1000000,)) # 传入key，进行随机数生成  key, subkey = jax.random.split(key) # 更新RNG State xx = jax.random.normal(subkey, shape=(1,))  key, subkey = jax.random.split(key) # 更新RNG State xxx = jax.random.normal(subkey, shape=(1,)) 第三个反例：\nxs = [1,2,3]  def not_pure_function_case3(xs):  xs.append(1.)  return xs  not_pure_function_case3(xs) # [1, 2, 3, 1.0]  not_pure_function_case3(xs) # [1, 2, 3, 1.0, 1.0] 函数内部修改了xs，违背了第二条原则。\n第四个反例：\ndef not_pure_function_case4(x):  print(\"oops, not pure\")  return x 这个反例是因为print属于IO操作，违背了第二条。\nNote: 如果我们不小心写出了non-pure function，然后进行transformation怎么办？你肯定指望JAX抛出一个异常，可惜的是，JAX内部并没有检查函数是否pure的机制，对于non-pure，transformation的行为属于undefined，有点像C语言中的野指针，此时函数的执行结果不可预测。\njaxpr 稍微聊一下transformation背后的故事，JAX中定义了一种中间表示语言（jaxpr），每个transformation的执行都分两步：\n 先将原Python函数翻译为jaxpr，这个过程被称为\"tracing\" 再对jaxpr进行transform (转换)，可以将每个transformation看作一个独立的jaxpr interpreter，对于JAX中每个原子操作 (primitive)都有相应的转换规则  jaxpr的优势是语法简单，相比于直接对Python函数transform，对jaxpr进行transform容易得多。\n 如何实现NN model 有了jax.numpy、grad、pmap、jit，现在就可以编写网络，实现训练过程了，但是想象下用NumPy实现一个ResNet，实现一个Transformer，能做，但是也太复杂了，\n 下一篇会介绍Flax，一个基于JAX的NN library，如何基于Flax+JAX来轻松实现网络训练流程。\n参考资料 [1] JAX文档，JAX reference documentation\n[2] JAX的两位creator Roy和Matt 对JAX项目的介绍，强烈推荐，Stanford MLSys Seminar Episode 6: Roy Frostig on JAX\n[3] JAX团队的Skye Wanderman-Milne 对JAX项目的介绍，0:42:49 Marc van Zee (Google Brain): Introduction to Flax\n[4] JHU的Sabrina J. Mielke对JAX的介绍，0:26:16 Sabrina J. Mielke (Johns Hopkins University \u0026 Hugging Face): From stateful code to purified JAX: how to build your neural net framework\n[5] JHU的Sabrina J. Mielke一篇博客，上一个链接的文字版，From PyTorch to JAX: towards neural net frameworks that purify stateful code\n[6] 前JAX团队Mat Kelcey对JAX的介绍，“High performance machine learning with JAX” - Mat Kelcey (PyConline AU 2021)\n[7] Matt对JAX的介绍，内容有点雷同，JAX: accelerated machine learning research via composable function transformations in Python\n[8] DeepMind团队介绍JAX在内部使用情况，NeurIPS 2020: JAX Ecosystem Meetup\n[9] Autograd项目，https://github.com/HIPS/autograd\n[10] XLA项目，https://www.tensorflow.org/xla\n[11] 如果想了解下随机数生成，强烈推荐该领域大牛 Melissa E. O’Neill写的 Random Number Generation Basics\n","wordCount":"823","inLanguage":"en","image":"https://basicv8vc.github.io/%3Cimage%20path/url%3E","datePublished":"2022-07-21T01:31:00Z","dateModified":"2022-07-21T01:31:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-1/"},"publisher":{"@type":"Organization","name":"Yet Another Blog","logo":{"@type":"ImageObject","url":"https://basicv8vc.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://basicv8vc.github.io/ accesskey=h title="basicv8vc (Alt + H)">basicv8vc</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://basicv8vc.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://basicv8vc.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://basicv8vc.github.io/posts/>Posts</a></div><h1 class=post-title>面向PyTorch用户的JAX简易教程[1]: JAX介绍</h1><div class=post-meta><span title="2022-07-21 01:31:00 +0000 +0000">July 21, 2022</span>&nbsp;·&nbsp;4 min</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e8%83%8c%e6%99%af aria-label=背景>背景</a></li><li><a href=#jax%e7%ae%80%e4%bb%8b aria-label=JAX简介>JAX简介</a><ul><li><a href=#xla aria-label=XLA>XLA</a></li><li><a href=#numpy aria-label=NumPy>NumPy</a></li><li><a href=#autograd aria-label=Autograd>Autograd</a></li></ul></li><li><a href=#composable-function-transformations-%e5%8f%af%e7%bb%84%e5%90%88%e7%9a%84%e5%87%bd%e6%95%b0%e8%bd%ac%e6%8d%a2 aria-label="Composable (function) transformations (可组合的函数转换)">Composable (function) transformations (可组合的函数转换)</a><ul><li><a href=#grad aria-label=grad>grad</a></li><li><a href=#jit aria-label=jit>jit</a></li><li><a href=#vmap aria-label=vmap>vmap</a></li><li><a href=#pmap aria-label=pmap>pmap</a></li><li><a href=#%e7%bb%84%e5%90%88 aria-label=组合>组合</a></li><li><a href=#%e7%ba%af%e5%87%bd%e6%95%b0%e7%ba%a6%e6%9d%9f aria-label=纯函数约束>纯函数约束</a></li></ul></li><li><a href=#jaxpr aria-label=jaxpr>jaxpr</a></li><li><a href=#%e5%a6%82%e4%bd%95%e5%ae%9e%e7%8e%b0nn-model aria-label="如何实现NN model">如何实现NN model</a></li><li><a href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99 aria-label=参考资料>参考资料</a></li></ul></div></details></div><div class=post-content><h1 id=背景>背景<a hidden class=anchor aria-hidden=true href=#背景>#</a></h1><p>前几天申请参加了<a href=https://basicv8vc.github.io/posts/trc-program/>Google TRC项目</a>，TPU VM的配置相当可以，但是PyTorch/XLA做数据并行时的体验却并不那么丝滑，考虑到Google一直力推TPU+JAX的组合，所以决定学习下JAX。</p><h1 id=jax简介>JAX简介<a hidden class=anchor aria-hidden=true href=#jax简介>#</a></h1><p>什么是JAX？</p><p>官方在GitHub README中是这么介绍的: JAX is <strong>Autograd</strong> and <strong>XLA</strong>, brought together for high-performance machine learning research.</p><p>在Description中写的是: <strong>Composable transformations</strong> of Python+<strong>NumPy</strong> programs: differentiate, vectorize, JIT to GPU/TPU, and more.</p><p>在JAX官方文档又是这么介绍的: JAX is <strong>NumPy</strong> on the <strong>CPU, GPU, and TPU</strong>, with great automatic differentiation for high-performance machine learning research.</p><p>总结一下，有几个关键词：Autograd、XLA、NumPy和composable transformations。</p><h2 id=xla>XLA<a hidden class=anchor aria-hidden=true href=#xla>#</a></h2><p>先来说<a href=https://developers.googleblog.com/2017/03/xla-tensorflow-compiled.html>XLA</a>，这个我了解的最少，所以介绍起来最简单，XLA (Accelerated Linear Algebra)是Google为TensorFlow设计的一款编译器，主打JIT (Just-in-Time)编译和跨设备（CPU/GPU/TPU）执行，所以JAX介绍中凡是涉及到JIT、high-performance、CPU/GPU/TPU，都指的是XLA。</p><h2 id=numpy>NumPy<a hidden class=anchor aria-hidden=true href=#numpy>#</a></h2><p>NumPy就不用提了，Python生态下只要涉及到数据分析/机器学习/数值计算中对数组/tensor进行操作，都离不开它，不夸张的说，NumPy API已经成为了数组/tensor操作的半个工业标准，包括各家深度学习框架中对tensor操作的函数接口也都是尽量靠近NumPy，JAX则更夸张，<code>jax.numpy</code>重新实现一套了NumPy API ，让用户从NumPy无缝切入JAX：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> jax <span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> jnp
</span></span></code></pre></div><h2 id=autograd>Autograd<a hidden class=anchor aria-hidden=true href=#autograd>#</a></h2><p>这里的Autograd是哈佛大学HIPS实验室在14年开始开发的一款自动微分框架，特点是可以对Python/NumPy函数进行高阶求导，直接看个例子，一个简单的函数 f(x) ，顺便求一下一阶、二阶、三阶导函数：</p><p>\[f(x) = x^3 + 2 \cdot x \]</p><p>\[f&rsquo;(x) = 3 \cdot x^2 + 2 \]</p><p>\[f&rsquo;&rsquo;(x) = 6 \cdot x \]</p><p>\[f&rsquo;&rsquo;&rsquo;(x) = 6 \]</p><p>如果\(x = 2 \) ，甚至可以口算出 \(f&rsquo;(2) = 14, f&rsquo;&rsquo;(2)=12, f&rsquo;&rsquo;&rsquo;(2)=6 \) 。我们可以用autograd来实现求导：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> autograd <span style=color:#f92672>import</span> grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x<span style=color:#f92672>**</span><span style=color:#ae81ff>3</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>grad_f <span style=color:#f92672>=</span> grad(f)  <span style=color:#75715e># 一阶导函数</span>
</span></span><span style=display:flex><span>grad_grad_f <span style=color:#f92672>=</span> grad(grad_f)  <span style=color:#75715e># 两次grad组合，就是二阶导函数</span>
</span></span><span style=display:flex><span>grad_grad_grad_f <span style=color:#f92672>=</span> grad(grad_grad_f)  <span style=color:#75715e># 三次grad组合，就是三阶导函数</span>
</span></span><span style=display:flex><span>print(grad_f(<span style=color:#ae81ff>2.</span>), grad_grad_f(<span style=color:#ae81ff>2.</span>), grad_grad_grad_f(<span style=color:#ae81ff>2.</span>))
</span></span><span style=display:flex><span><span style=color:#75715e># 14.0 12.0 6.0</span>
</span></span></code></pre></div><p>自动微分框架除了可以应用于数值计算，它还是深度学习框架的核心，可惜的是，由于性能（纯Python，只有CPU版本）以及其他原因，autograd库并没有推广起来，但是它却实实在在启发到了后续的torch-autograd、Chainer以及PyTorch中的autograd模块：</p><figure class=align-center><img loading=lazy src=/images/jax_tutorials_1/matt_adam_discuss.jpeg#center></figure><p><code>注：Adam毕业后加入了JAX团队，PyTorch在1.10版本也推出了functorch (JAX-like composable function transforms for PyTorch), 他们都有光明的未来:)</code></p><p>估计是Matthew一直对autograd性能耿耿于怀，当他在Google内部听到XLA的分享后，便和同事产生了JAX的最初想法：</p><p><strong>Autograd + XLA ===> JAX</strong></p><p>前者负责微分功能，后者实现高性能。</p><p><code>注：可以将JAX中的算子(operation，操作)看做是对XLA算子的Python封装：jax.numpy中的操作/算子是对更底层的jax.lax的封装，而jax.lax中的算子是XLA算子的Python封装。</code></p><h1 id=composable-function-transformations-可组合的函数转换>Composable (function) transformations (可组合的函数转换)<a hidden class=anchor aria-hidden=true href=#composable-function-transformations-可组合的函数转换>#</a></h1><p>composable transformations是JAX的核心，真正体现了JAX的特性/差异/优势。 // <em>标题都改成一级标题了。</em></p><p>什么是transformation (function transformations, transforms)？其实就是高阶函数 (Higher-order function)，高阶函数是至少满足下列一个条件的函数：</p><ul><li>接受一个或多个函数作为输入</li><li>输出一个函数</li></ul><p>Python中常见的高阶函数比如<code>map</code>：</p><figure class=align-center><img loading=lazy src=/images/jax_tutorials_1/map_demo.jpeg#center></figure><p>transformation的输入是Python函数，输出也是函数。JAX中经常用到的transformation主要有四个：</p><ul><li><strong>grad</strong>: reverse mode自动微分，用在深度学习中足够了</li><li><strong>jit</strong>: JIT编译，调用XLA进行JIT编译，用于优化代码</li><li><strong>vmap</strong>: vectorization/batching，将函数扩展为支持批处理</li><li><strong>pmap</strong>: parallelization，轻松实现数据并行 (data parallelism)，类似PyTorch的<code>DistributedDataParallel</code></li></ul><p>不知道看到这里，你是不是会很疑惑，JAX的核心就是这么几个高阶函数？能干啥？</p><figure class=align-center><img loading=lazy src=/images/jax_tutorials_1/why.jpeg#center width=400px></figure><p>我们来看下这四个transformation到底能干啥？</p><h2 id=grad>grad<a hidden class=anchor aria-hidden=true href=#grad>#</a></h2><p><code>grad</code>在Autograd那里已经介绍过了，</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> jax <span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> jnp
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> jax <span style=color:#f92672>import</span> grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> jnp<span style=color:#f92672>.</span>sum(x <span style=color:#f92672>*</span> x)  <span style=color:#75715e># 函数输出只能是标量</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>grad_f <span style=color:#f92672>=</span> grad(f)
</span></span><span style=display:flex><span>grad_f(jnp<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3.</span>]))
</span></span><span style=display:flex><span><span style=color:#75715e># DeviceArray([2., 4., 6.], dtype=float32)</span>
</span></span></code></pre></div><p><code>grad</code>只是JAX自动微分机制中最基本的一个transform，实际上JAX支持前向(forward-mode)和后向(reverse-mode)自动微分以及二者的任意组合， 感兴趣的同学可以去查看<a href=https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#vector-jacobian-products-vjps-aka-reverse-mode-autodiff>jvp和vjp</a> 的文档。考虑到常见的深度学习任务，grad戳戳有余， 其他transform这里就不介绍了，<em>实际上是我没用过，压根没那个能力介绍。</em></p><p><code>grad</code>不但好用，而且数学上更直观，如果我们不局限在深度学习领域，从优化 (optimization)的角度看，大多数机器学习模型的学习都可以表示为\(\hat{y} =f(x)\) 、\(\max_{{y}}\ p(y|x)\) 、\(\max_{y} \ \frac{p(x, y)}{p(x)}\) 的一种。</p><p>LR可以表示为\(f(x)\)，神经网络也可以表示为\(f(x)\)，损失函数是\(loss = g(f(x), y)\)，如果用SGD算法来解决，需要计算参数的梯度，想一下高数课上我们是怎么做的，直接对损失函数求导函数\(grad(g)\) 然后代入\(x\) ，现在grad用的就是这种方式。并且这种方式在数学上可以自然的泛化到高阶导数优化求解问题上。</p><h2 id=jit>jit<a hidden class=anchor aria-hidden=true href=#jit>#</a></h2><p><code>jit</code> 是用户显式的调用XLA对代码进行优化（包括算子融合、内存优化等），执行时间可能缩短很多，</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> jax <span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> jnp
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> jax <span style=color:#f92672>import</span> jit
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>norm</span>(X):
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> X <span style=color:#f92672>-</span> X<span style=color:#f92672>.</span>mean(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> X <span style=color:#f92672>/</span> X<span style=color:#f92672>.</span>std(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>norm_compiled <span style=color:#f92672>=</span> jit(norm)
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>array(np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>10000</span>, <span style=color:#ae81ff>100</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>timeit norm(X)<span style=color:#f92672>.</span>block_until_ready()
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>timeit norm_compiled(X)<span style=color:#f92672>.</span>block_until_ready()
</span></span><span style=display:flex><span><span style=color:#75715e># 585 µs ± 85.4 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 216 µs ± 12.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 好像提升不是很显著，再来看一个例子</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> jax <span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>key <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>PRNGKey(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>selu</span>(x, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>1.67</span>, lmbda<span style=color:#f92672>=</span><span style=color:#ae81ff>1.05</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> lmbda <span style=color:#f92672>*</span> jnp<span style=color:#f92672>.</span>where(x <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>, x, alpha <span style=color:#f92672>*</span> jnp<span style=color:#f92672>.</span>exp(x) <span style=color:#f92672>-</span> alpha)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>selu_jit <span style=color:#f92672>=</span> jit(selu)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>normal(key, (<span style=color:#ae81ff>1000000</span>,))
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>timeit selu(x)<span style=color:#f92672>.</span>block_until_ready()
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>timeit selu_jit(x)<span style=color:#f92672>.</span>block_until_ready()
</span></span><span style=display:flex><span><span style=color:#75715e># 1.06 ms ± 26.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 187 µs ± 19.6 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 哦豁，效果还不错</span>
</span></span></code></pre></div><h2 id=vmap>vmap<a hidden class=anchor aria-hidden=true href=#vmap>#</a></h2><p><code>vmap</code>可以自动让函数支持batching，看个例子，原始函数表示的是向量-向量乘法，使用<code>vmap</code>可以得到矩阵-向量乘法的函数：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> jax <span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> jnp
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> jax <span style=color:#f92672>import</span> vmap
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>vec_vec_dot</span>(x, y):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;vector-vector dot, ([a], [a]) -&gt; []
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> jnp<span style=color:#f92672>.</span>dot(x, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>])
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>,])
</span></span><span style=display:flex><span>vec_vec_dot(x, y)
</span></span><span style=display:flex><span><span style=color:#75715e># DeviceArray(5, dtype=int32)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mat_vec <span style=color:#f92672>=</span> vmap(vec_vec_dot, in_axes<span style=color:#f92672>=</span>(<span style=color:#ae81ff>0</span>, <span style=color:#66d9ef>None</span>), out_axes<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)  <span style=color:#75715e># ([b,a], [a]) -&gt; [b]      (b is the mapped axis)</span>
</span></span><span style=display:flex><span>xx <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>], [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>]])
</span></span><span style=display:flex><span>mat_vec(xx, y)
</span></span><span style=display:flex><span><span style=color:#75715e># DeviceArray([5, 5], dtype=int32)</span>
</span></span></code></pre></div><p>解释下vmap中的<code>in_axes</code>和<code>out_axees</code>两个参数，前者表示对输入参数中哪一个的哪一维度进行batch扩充，这里<code>(0, None)</code>表示对<code>x</code>的第0维扩充，由原来的<code>[a] -> [b,a]</code>。后者表示对返回结果的哪一维度进行扩充，这里表示由原来的<code>[] - > [b]</code>。</p><h2 id=pmap>pmap<a hidden class=anchor aria-hidden=true href=#pmap>#</a></h2><p><code>pmap</code>让并行编程变的非常丝滑，可以用于数据并行训练，注意<code>pmap</code>包含了<code>jit</code>操作，下面我就在TPU v3-8 VM演示下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> jax
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> jax <span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> jnp
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> jax <span style=color:#f92672>import</span> pmap
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>jax<span style=color:#f92672>.</span>device_count()  <span style=color:#75715e># 8个core</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 8</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>jax<span style=color:#f92672>.</span>devices()
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),
</span></span></span><span style=display:flex><span><span style=color:#e6db74> TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),
</span></span></span><span style=display:flex><span><span style=color:#e6db74> TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),
</span></span></span><span style=display:flex><span><span style=color:#e6db74> TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),
</span></span></span><span style=display:flex><span><span style=color:#e6db74> TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),
</span></span></span><span style=display:flex><span><span style=color:#e6db74> TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),
</span></span></span><span style=display:flex><span><span style=color:#e6db74> TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),
</span></span></span><span style=display:flex><span><span style=color:#e6db74> TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>vmap(jnp<span style=color:#f92672>.</span>add)(x, y)
</span></span><span style=display:flex><span><span style=color:#75715e># DeviceArray([ 0,  2,  4,  6,  8, 10, 12, 14], dtype=int32)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pmap(jnp<span style=color:#f92672>.</span>add)(x, y)
</span></span><span style=display:flex><span><span style=color:#75715e># ShardedDeviceArray([ 0,  2,  4,  6,  8, 10, 12, 14], dtype=int32)</span>
</span></span></code></pre></div><p>看到上面vmap和pmap执行后的区别没，一个返回数据类型是DeviceArray，一个则是SharedDeviceArray，后者表示数据分散在多个device中。</p><h2 id=组合>组合<a hidden class=anchor aria-hidden=true href=#组合>#</a></h2><p>上面介绍的transformation不仅仅可以单兵作战，最重要的是可以任意组合，比如</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pmap(vamp(some_func))
</span></span><span style=display:flex><span>jit(grad(grad(vmap(some_func))))
</span></span></code></pre></div><h2 id=纯函数约束>纯函数约束<a hidden class=anchor aria-hidden=true href=#纯函数约束>#</a></h2><p>transformation很好用，但是只能作用于纯函数 (pure function)。</p><p><code>或者反过来理解，正因为函数都是纯函数，才可以实现composable transformations这样灵活强大的功能。</code></p><p>什么是纯函数？</p><ol><li>只要函数的传参不变，函数返回结果就要相同</li><li>函数不会改变函数外的状态</li></ol><p>我们直接来看<strong>反例</strong>吧，第一个反例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> <span style=color:#ae81ff>3.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>not_pure_function_case1</span>(a):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x <span style=color:#f92672>+</span> a
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(not_pure_function_case1(<span style=color:#ae81ff>1.</span>))
</span></span><span style=display:flex><span><span style=color:#75715e># 4.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> <span style=color:#ae81ff>5.</span>
</span></span><span style=display:flex><span>print(not_pure_function_case1(<span style=color:#ae81ff>1.</span>))
</span></span><span style=display:flex><span><span style=color:#75715e># 6.</span>
</span></span></code></pre></div><p>我们使用相同的传参(1.)调用了两次，可是函数结果不同，所以违背了第一条原则。这是因为函数内部使用了全局变量，虽然仅仅是read value，但是只要全局变量的值改变，函数返回结果就变了。</p><p><code>Tip: 纯函数内部不要读或写函数外的变量。</code></p><p>第二个反例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>123</span>)  <span style=color:#75715e># 设置随机数种子</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>not_pure_function_case2</span>(n):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(n)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>not_pure_function_case2(<span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># array([-1.0856306 ,  0.99734545,  0.2829785 , -1.50629471, -0.57860025])</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>not_pure_function_case2(<span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># array([ 1.65143654, -2.42667924, -0.42891263,  1.26593626, -0.8667404 ])</span>
</span></span></code></pre></div><p>随机数在机器学习中太常见了，你看，为了结果可复现，我们还设置了随机数种子，但是，这却不是一个纯函数。</p><p>在NumPy中，随机数生成器状态(RNG State)是一个全局变量，只要我们调用了随机数生成算法（比如上面的np.random.rand()），都会导致RNG State发生变化，这样，连续两次的随机数生成结果就不相同，又违背了纯函数第一条原则。</p><p>为此，<code>jax.numpy</code>和<code>numpy</code>的第一个不同之处出现了，JAX没有隐含的全局RNG State，凡是涉及到随机数生成的地方，都需要用户显式的使用RNG State。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> jax
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>PRNGKey(<span style=color:#ae81ff>0</span>)  <span style=color:#75715e># 显式的创建PRNGKey，可以表示RNG State</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(key, (<span style=color:#ae81ff>1000000</span>,))  <span style=color:#75715e># 传入key，进行随机数生成</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>key, subkey <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)  <span style=color:#75715e># 更新RNG State</span>
</span></span><span style=display:flex><span>xx <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(subkey, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>key, subkey <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)  <span style=color:#75715e># 更新RNG State</span>
</span></span><span style=display:flex><span>xxx <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(subkey, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,))
</span></span></code></pre></div><p>第三个反例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>xs <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>not_pure_function_case3</span>(xs):
</span></span><span style=display:flex><span>    xs<span style=color:#f92672>.</span>append(<span style=color:#ae81ff>1.</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> xs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>not_pure_function_case3(xs)
</span></span><span style=display:flex><span><span style=color:#75715e># [1, 2, 3, 1.0]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>not_pure_function_case3(xs)
</span></span><span style=display:flex><span><span style=color:#75715e># [1, 2, 3, 1.0, 1.0]</span>
</span></span></code></pre></div><p>函数内部修改了<code>xs</code>，违背了第二条原则。</p><p>第四个反例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>not_pure_function_case4</span>(x):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;oops, not pure&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><p>这个反例是因为<code>print</code>属于IO操作，违背了第二条。</p><p><strong>Note:</strong> 如果我们不小心写出了non-pure function，然后进行transformation怎么办？你肯定指望JAX抛出一个异常，可惜的是，JAX内部并没有检查函数是否pure的机制，对于non-pure，transformation的行为属于undefined，有点像C语言中的野指针，此时函数的执行结果不可预测。</p><h1 id=jaxpr>jaxpr<a hidden class=anchor aria-hidden=true href=#jaxpr>#</a></h1><p>稍微聊一下transformation背后的故事，JAX中定义了一种中间表示语言（jaxpr），每个transformation的执行都分两步：</p><ol><li>先将原Python函数翻译为jaxpr，这个过程被称为"tracing"</li><li>再对jaxpr进行transform (转换)，可以将每个transformation看作一个独立的jaxpr interpreter，对于JAX中每个原子操作 (primitive)都有相应的转换规则</li></ol><p>jaxpr的优势是语法简单，相比于直接对Python函数transform，对jaxpr进行transform容易得多。</p><figure class=align-center><img loading=lazy src=/images/jax_tutorials_1/jaxpr.png#center width=600px></figure><h1 id=如何实现nn-model>如何实现NN model<a hidden class=anchor aria-hidden=true href=#如何实现nn-model>#</a></h1><p>有了jax.numpy、grad、pmap、jit，现在就可以编写网络，实现训练过程了，但是想象下用NumPy实现一个ResNet，实现一个Transformer，能做，但是也太复杂了，</p><figure class=align-center><img loading=lazy src=/images/jax_tutorials_1/stop-talking.jpeg#center width=300px></figure><p>下一篇会介绍Flax，一个基于JAX的NN library，如何基于Flax+JAX来轻松实现网络训练流程。</p><h1 id=参考资料>参考资料<a hidden class=anchor aria-hidden=true href=#参考资料>#</a></h1><p>[1] JAX文档，<a href=https://jax.readthedocs.io/en/latest/index.html>JAX reference documentation</a></p><p>[2] JAX的两位creator Roy和Matt 对JAX项目的介绍，强烈推荐，<a href="https://www.youtube.com/watch?v=mbUwCPiqZBM&list=PLMIKHILlF7N3UTXM9fJY-qyXU7VxJUin5&index=2&t=2845s&ab_channel=StanfordMLSysSeminars">Stanford MLSys Seminar Episode 6: Roy Frostig on JAX</a></p><p>[3] JAX团队的Skye Wanderman-Milne 对JAX项目的介绍，<a href="https://www.youtube.com/watch?v=fuAyUQcVzTY&t=364s&ab_channel=HuggingFace">0:42:49 Marc van Zee (Google Brain): Introduction to Flax</a></p><p>[4] JHU的Sabrina J. Mielke对JAX的介绍，<a href="https://www.youtube.com/watch?v=__eG63ZP_5g&t=2289s&ab_channel=HuggingFace">0:26:16 Sabrina J. Mielke (Johns Hopkins University & Hugging Face): From stateful code to purified JAX: how to build your neural net framework</a></p><p>[5] JHU的Sabrina J. Mielke一篇博客，上一个链接的文字版，<a href=https://sjmielke.com/jax-purify.htm>From PyTorch to JAX: towards neural net frameworks that purify stateful code</a></p><p>[6] 前JAX团队Mat Kelcey对JAX的介绍，<a href="https://www.youtube.com/watch?v=cqbBjM4_yGw&list=PLMIKHILlF7N3UTXM9fJY-qyXU7VxJUin5&index=9&ab_channel=PyConAU">&ldquo;High performance machine learning with JAX&rdquo; - Mat Kelcey (PyConline AU 2021)</a></p><p>[7] Matt对JAX的介绍，内容有点雷同，<a href="https://www.youtube.com/watch?v=BzuEGdGHKjc&list=PLMIKHILlF7N3UTXM9fJY-qyXU7VxJUin5&index=12&t=2550s&ab_channel=ACMSIGPLAN">JAX: accelerated machine learning research via composable function transformations in Python</a></p><p>[8] DeepMind团队介绍JAX在内部使用情况，<a href="https://www.youtube.com/watch?v=iDxJxIyzSiM&list=PLMIKHILlF7N3UTXM9fJY-qyXU7VxJUin5&index=14&t=2433s&ab_channel=DeepMind">NeurIPS 2020: JAX Ecosystem Meetup</a></p><p>[9] Autograd项目，https://github.com/HIPS/autograd</p><p>[10] XLA项目，https://www.tensorflow.org/xla</p><p>[11] 如果想了解下随机数生成，强烈推荐该领域大牛 Melissa E. O&rsquo;Neill写的 <a href=https://www.pcg-random.org/rng-basics.html>Random Number Generation Basics</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://basicv8vc.github.io/tags/jax/>JAX</a></li><li><a href=https://basicv8vc.github.io/tags/tpu/>TPU</a></li><li><a href=https://basicv8vc.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li></ul><nav class=paginav><a class=prev href=https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-2/><span class=title>« Prev Page</span><br><span>面向PyTorch用户的JAX简易教程[2]: 如何训练一个神经网络</span></a>
<a class=next href=https://basicv8vc.github.io/posts/sentence-bert/><span class=title>Next Page »</span><br><span>Sentence-BERT: 如何通过对比学习得到更好的句子向量表示</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 面向PyTorch用户的JAX简易教程[1]: JAX介绍 on twitter" href="https://twitter.com/intent/tweet/?text=%e9%9d%a2%e5%90%91PyTorch%e7%94%a8%e6%88%b7%e7%9a%84JAX%e7%ae%80%e6%98%93%e6%95%99%e7%a8%8b%5b1%5d%3a%20JAX%e4%bb%8b%e7%bb%8d&url=https%3a%2f%2fbasicv8vc.github.io%2fposts%2fjax-tutorials-for-pytorchers-1%2f&hashtags=JAX%2cTPU%2c%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 面向PyTorch用户的JAX简易教程[1]: JAX介绍 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fbasicv8vc.github.io%2fposts%2fjax-tutorials-for-pytorchers-1%2f&title=%e9%9d%a2%e5%90%91PyTorch%e7%94%a8%e6%88%b7%e7%9a%84JAX%e7%ae%80%e6%98%93%e6%95%99%e7%a8%8b%5b1%5d%3a%20JAX%e4%bb%8b%e7%bb%8d&summary=%e9%9d%a2%e5%90%91PyTorch%e7%94%a8%e6%88%b7%e7%9a%84JAX%e7%ae%80%e6%98%93%e6%95%99%e7%a8%8b%5b1%5d%3a%20JAX%e4%bb%8b%e7%bb%8d&source=https%3a%2f%2fbasicv8vc.github.io%2fposts%2fjax-tutorials-for-pytorchers-1%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 面向PyTorch用户的JAX简易教程[1]: JAX介绍 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fbasicv8vc.github.io%2fposts%2fjax-tutorials-for-pytorchers-1%2f&title=%e9%9d%a2%e5%90%91PyTorch%e7%94%a8%e6%88%b7%e7%9a%84JAX%e7%ae%80%e6%98%93%e6%95%99%e7%a8%8b%5b1%5d%3a%20JAX%e4%bb%8b%e7%bb%8d"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 面向PyTorch用户的JAX简易教程[1]: JAX介绍 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbasicv8vc.github.io%2fposts%2fjax-tutorials-for-pytorchers-1%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 面向PyTorch用户的JAX简易教程[1]: JAX介绍 on whatsapp" href="https://api.whatsapp.com/send?text=%e9%9d%a2%e5%90%91PyTorch%e7%94%a8%e6%88%b7%e7%9a%84JAX%e7%ae%80%e6%98%93%e6%95%99%e7%a8%8b%5b1%5d%3a%20JAX%e4%bb%8b%e7%bb%8d%20-%20https%3a%2f%2fbasicv8vc.github.io%2fposts%2fjax-tutorials-for-pytorchers-1%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 面向PyTorch用户的JAX简易教程[1]: JAX介绍 on telegram" href="https://telegram.me/share/url?text=%e9%9d%a2%e5%90%91PyTorch%e7%94%a8%e6%88%b7%e7%9a%84JAX%e7%ae%80%e6%98%93%e6%95%99%e7%a8%8b%5b1%5d%3a%20JAX%e4%bb%8b%e7%bb%8d&url=https%3a%2f%2fbasicv8vc.github.io%2fposts%2fjax-tutorials-for-pytorchers-1%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=basicv8vc/basicv8vc.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://basicv8vc.github.io/>Yet Another Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>