<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Yet Another Blog</title>
    <link>https://basicv8vc.github.io/posts/</link>
    <description>Recent content in Posts on Yet Another Blog</description>
    <image>
      <url>https://basicv8vc.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://basicv8vc.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 05 Jun 2022 22:50:29 +0800</lastBuildDate><atom:link href="https://basicv8vc.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>图解OneFlow中的学习率调整策略</title>
      <link>https://basicv8vc.github.io/posts/oneflow-lrscheduler-visualization/</link>
      <pubDate>Sun, 05 Jun 2022 22:50:29 +0800</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/oneflow-lrscheduler-visualization/</guid>
      <description>背景 学习率调整策略（learning rate scheduler），其实单独拎出每一个来看都不难，但是由于方法较多，上来就看文档容易一头雾水， 以OneFlow v0.7.0为例，oneflow.optim.lr_scheduler模块中就包含了14种策略。
有没有一种更好的方法来学习呢？比如可视化出学习率的变化过程，此时，我脑海中突然浮现出Convolution arithmetic这个经典项目，作者将各种CNN卷积操作以gif形式展示，一目了然。
 所以，就有了这篇文章，将学习率调整策略可视化出来，下面是两个例子（ConstantLR和LinearLR）：
  我将可视化代码分别托管在Hugging Face Spaces和Streamlit Cloud，大家可以任选一个链接访问，然后自由调节参数，感受学习率的变化过程。
Hugging Face Space版本
Streamlit Cloud版本
学习率调整策略 学习率可以说是训练神经网络过程中最重要的参数（之一），目前大家都已接受用动态学习率调整策略来代替固定学习率，各种学习率调整策略层出不穷，下面我们就以OneFlow v0.7.0为例，学习下常用的几种策略。
基类LRScheduler LRScheduler(optimizer: Optimizer, last_step: int = -1, verbose: bool = False)是所有学习率调度器的基类，初始化参数中last_step和verbose一般不需要设置，前者主要和checkpoint相关，后者则是打印当前学习率。LRScheduler中最重要的方法是step()，这个方法的作用就是修改用户设置的初始学习率，然后应用到下一次的Optimizer.step()。
有些资料会讲LRScheduler根据epoch或iteration/step来调整学习率，两种说法都没问题，实际上，LRScheduler并不知道当前训练到第几个epoch或第几个iteration/step，只记录了调用step()的次数（last_step），如果每个epoch调用一次，那就是根据epoch来调整学习率，如果每个mini-batch 调用一次，那就是根据iteration来调整学习率，后者相对来说更常用。
简单来说，LRScheduler根据调整策略本身、当前调用step()的次数（last_step）和用户设置的初始学习率来得到下一次梯度更新时的学习率。
ConstantLR oneflow.optim.lr_scheduler.ConstantLR(  optimizer: Optimizer,  factor: float = 1.0 / 3,  total_iters: int = 5,  last_step: int = -1,  verbose: bool = False  ) ConstantLR和固定学习率差不多，唯一的区别是在前total_iters，学习率为初始学习率 * factor。</description>
    </item>
    
    <item>
      <title>DeepSpeed之ZeRO系列：将显存优化进行到底</title>
      <link>https://basicv8vc.github.io/posts/zero/</link>
      <pubDate>Sat, 14 May 2022 23:20:54 +0800</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/zero/</guid>
      <description>前言 目前训练超大规模语言模型主要有两条技术路线：TPU + XLA + TensorFlow/JAX 和 GPU + PyTorch + Megatron-LM + DeepSpeed。前者由Google主导，由于TPU和自家云平台GCP深度绑定，对于非Googler来说， 只可远观而不可把玩，后者背后则有NVIDIA、Meta、MS大厂加持，社区氛围活跃，也更受到群众欢迎。
上面提到的DeepSpeed的核心是ZeRO(Zero Redundancy Optimizer)，简单来说，它是一种显存优化的数据并行(data parallelism, DP)方案。而“优化“这个话题又永无止境，在过去两年DeepSpeed团队发表了三篇ZeRO相关的论文，提出了去除冗余参数、引入CPU和内存、引入NVMe等方法，从始至终都围绕着一个目标：将显存优化进行到底。
ZeRO: 一种去除冗余的数据并行方案 ZeRO: Memory Optimizations Toward Training Trillion Parameter Models 发表在SC 20，DeepSpeed项目最初就是论文中ZeRO方法的官方实现。
背景 如今训练大模型离不开各种分布式并行策略，常用的并行策略包括：
 数据并行（data parallelism, DP）：假设有 \(N\) 张卡，每张卡都保存一个模型，每一次迭代（iteration/step）都将batch数据分割成 \(N\) 个等大小的micro-batch，每张卡根据拿到的micro-batch数据独立计算梯度，然后调用AllReduce计算梯度均值，每张卡再独立进行参数更新。  # https://huggingface.co/docs/transformers/parallelism#model-parallelism # 假设模型有三层：L0, L1, L2 # 每层有两个神经元 # 两张卡  GPU0: L0 | L1 | L2 ---|----|--- a0 | b0 | c0 a1 | b1 | c1  GPU1: L0 | L1 | L2 ---|----|--- a0 | b0 | c0 a1 | b1 | c1  模型并行（model parallelism/tensor parallelism, MP/TP）：有的tensor/layer很大，一张卡放不下，将tensor分割成多块，一张卡存一块。  # https://huggingface.</description>
    </item>
    
    <item>
      <title>Hugging Face🤗 : NLP明星创业公司是如何炼成的</title>
      <link>https://basicv8vc.github.io/posts/huggingface/</link>
      <pubDate>Thu, 21 Apr 2022 19:09:35 +0800</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/huggingface/</guid>
      <description>2016年，两位法国人怀揣着对聊天机器人(chatbot)行业未来的美好向往，创立了Hugging Face 🤗，次年推出了一款和公司同名的机器人聊天App。最初，他们只是想为无聊的年轻人提供一个打发时间的地方，&amp;quot;Its entire purpose is to be fun&amp;quot; 可是不论是从技术难度还是商业模式上看，在2016年的时间点选择做开放领域的闲聊机器人，都不是一个明智选择。 时间来到了2018年10月29号，这一天公司首席科学家Thomas Wolf在GitHub上创建了一个名为pytorch-pretrained-BERT的项目，提交了第一个commit，这个项目最初的目的是如此简单，将Google基于tensorflow实现的BERT模型用pytorch进行了重写，并且可以加载Google公开的模型参数。但是，项目的火热程度超出了所有人的预期，Hugging Face也在开源模式中一路高歌猛进，顺利完成了公司转型，成为了当下NLP领域最火的创业公司（之一）。
入局聊天机器人 现在当你打开Hugging Face的官网，映入眼帘的都是&amp;quot;AI community&amp;quot;、&amp;ldquo;SOTA models&amp;rdquo;、&amp;ldquo;Open Source&amp;quot;等词汇，或许很多人都不知道，这曾经是一家做聊天机器人(chatbot)的公司，而且做的还是开放领域(open domain)的闲聊机器人，对，就是类似微软小冰，让我们把时间线拨回2016年。
Clem Delangue1和Julien Chaumond2一起参加了风投机构Betaworks举办的Botcamp加速器项目，专门为聊天机器人创业公司提供指导，在此期间（或者更早？）他们俩创立了Hugging Face3，研发一款为青少年打发时间找点乐子的陪聊型机器人，不论是公司名字（来自于emoji表情）还是产品，画风都既不商业(Enterprise)也不严肃，从这段时间的PR来看，他们坚信AI friend是大有未来的，We really have this vision where we think everyone will have an AI friend and everyone will discuss things every day with Hugging Face, so that’s really what we’re focused on right now and for the next few years4
 左边 是CTO Julien Chaumond，右边 是CEO Clem Delangue   在2017年3月9号，他们在iOS App Store正式推出了Hugging Face App，产品的界面是这样的：</description>
    </item>
    
    <item>
      <title>K-Means从入门到调包</title>
      <link>https://basicv8vc.github.io/posts/k-means%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%B0%83%E5%8C%85/</link>
      <pubDate>Tue, 19 Jul 2016 11:12:58 +0000</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/k-means%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%B0%83%E5%8C%85/</guid>
      <description>在做数据挖掘项目时，拿到数据，首先要分析问题和观察数据，我在“观察数据”这一阶段主要是看看有没有直观的数据规律以及异常点，对于那些特征维度较低的数据，比如仅仅2维，我选择画散点图观察，高纬度数据可以用t-SNE降维后再画图。除了可视化，聚类算法也是找到异常点的常用手段之一。
这篇博客介绍一种最基本的聚类算法：K-Means。
算法描述与EM 聚类算法的作用是将数据集自动划分为多个不同的集合。 K-Means算法是典型的无监督算法，对于数据集\(D\), 我们设定参数k(k个集合)，她就能自动地学习出k个类别，简单又神奇。那么K-Means是如何学习到这k个集合呢？专业一点说就是，K-Means的目标函数是啥？如何优化她的目标函数？先回答第一个问题，下面就是K-Means的目标函数：
物理意义：我们希望找到的k个集合，每个集合内的点都距离此集合内平均值点最近(每个集合中的点都紧密团结在一起)。
知道了目标函数，下一步就要考虑如何求解，最常见的求解方法是Lloyd算法，也就是使用EM算法，注意，Lloyd算法得到的结果是局部极小值而不是全局最小值哦。
EM算法求解步骤很简单，总共分三步：
 将所有数据点都分配到某个集合 -&amp;gt; 计算每个点到集合平均值点的欧氏距离 计算每个集合内点的平均值 -&amp;gt; 计算每个特征的平均值 重复上面两步直到局部收敛 -&amp;gt; 前后两次迭代的集合结果相同  上面的三步概括起来就是 E-&amp;gt;M-&amp;gt;E-&amp;gt;M-&amp;gt;&amp;hellip;&amp;hellip;&amp;hellip;..
import numpy as np from collections import defaultdict from random import uniform class KMeans(object): &amp;#34;&amp;#34;&amp;#34; kmeans 聚类算算 &amp;#34;&amp;#34;&amp;#34; def __init__(self): pass def cluster(self, X, k, initializer=&amp;#34;random&amp;#34;): self.X = X self.k = k self.centers = defaultdict(list) self.assignment = np.zeros(self.X.shape[0]) #记录每个样本属于哪个中心点 if initializer == &amp;#34;random&amp;#34;: self._initialize() #初始化得到k个点 elif initializer == &amp;#34;kmeans++&amp;#34;: self.</description>
    </item>
    
    <item>
      <title>Gradient Boosting 从入门到调包 (调包篇)</title>
      <link>https://basicv8vc.github.io/posts/gradient-boosting-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%B0%83%E5%8C%85-%E8%B0%83%E5%8C%85%E7%AF%87/</link>
      <pubDate>Sat, 09 Jul 2016 21:36:18 +0000</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/gradient-boosting-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%B0%83%E5%8C%85-%E8%B0%83%E5%8C%85%E7%AF%87/</guid>
      <description>Gradient Boosting的开源实现很多，当然首选还是XGBoost。调包，就是学习API的过程，最好的学习资料就是官方文档以及demo。我这里把基本的调用罗列出来:
xgboost调包之 基础篇.ipynb
xgboost调包之 模型篇.ipynb
xgboost调包之 sklearn接口.ipynb
xgboost调包之 特征重要性可视化.ipynb
xgboost调包之 early stopping.ipynb
xgboost调包之 结合pandas.ipynb</description>
    </item>
    
    <item>
      <title>Gradient Boosting 从入门到调包 (入门篇)</title>
      <link>https://basicv8vc.github.io/posts/gradient-boosting-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%B0%83%E5%8C%85-%E5%85%A5%E9%97%A8%E7%AF%87/</link>
      <pubDate>Fri, 08 Jul 2016 21:36:18 +0000</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/gradient-boosting-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%B0%83%E5%8C%85-%E5%85%A5%E9%97%A8%E7%AF%87/</guid>
      <description>开篇 虽然数据挖掘算法很多，但有过实际经验的同学肯定都知道，集成方法(ensembel method)大多数情况下都是首选。集成方法和LR、SVM算法不同，她不是一个具体的模型或算法，而是一类算法的统称。集成，从其字面意思也能看出，这家伙不是单兵作战，而是依靠团队取胜。简而言之，集成方法就是通过构建并结合多个学习器来完成学习任务。这句话有两个地方需要注意，“如何构建每一个学习器”以及“如何结合多个学习器而产生预测结果”。
如果我们按照“如何构建每一个学习器”来将集成方法分类，总体可以分为两类：1) 个体学习器减存在强依赖关系、必须串行生成的序列化方法，代表方法是Boosting；2）个体学习器之间不存在强依赖关系、可同时生成的并行化方法，代表方法是Bagging。
本文要讲到的Gradient Boosting就属于Boosting方法。
由于Gradient Boosting涉及到的知识实在是很多，完全可以写成类似&amp;quot;Understanding Random Forest: From Theory to Practice&amp;quot;的博士论文，由于水平和时间的限制，本文不会涉及过多、过深的细节，而是尽量用通俗的自然语言和数学语言来解释我对Gradient Boosting的浅显理解，仅仅是入门水平:)
决策树 既然要讲Gradient Boosting，那么决策树肯定是绕不开的，虽然Boosting方法对&amp;quot;个体学习器&amp;quot;没有要求，你可以选择LR、SVM、决策树等等。但是大多数情况下我们默认都选择决策树作为个体学习器。下面简单解释决策树算法。
决策树，顾名思义，根据树结构来进行决策。这种思想是很质朴的，和我们人类做决策时的思考过程很像，比如明天出门要不要带伞啊？如果我根据天气情况来做出决定，可以记作：
if 明天下雨： 出门带伞 else: 出门不带伞 看到没有，决策过程可以用if, else来具体化，难怪有人说，决策树就是一堆if else而已。数据挖掘比赛中很多人会首先用简单的规则跑出一个结果，这实际上就是在人工构造决策树啊。
既然决策树是一种机器学习方法，我们就想知道她的学习过程是怎么样的，也就是给定训练集如果构建一颗决策树呢？通常，决策树的构建是一个递归过程:
 1 输入训练集D，特征集F 2 如果D中样本全属于同一个类别C，将当前节点标记为C类叶节点，返回 3 从F中选择最优划分特征f 4 对于f的每一个值\(f_i\), 都生成一个子节点，然后对D进行划分，f值相同的样本都放到同一个子节点，并对节点进行标记，然后对于每一个新产生的子节点，进行第1步  注意：实际上对于决策树，大多数的实现方法都是基于二叉树，比如sklearn。
划分选择 决策树算法的关键是上面的第3步，即如何选择最优划分特征。如何评价一个特征是不是优秀呢？基本的原则就是看按照某一个特征生成子节点后，子节点的样本是不是都尽可能属于同一个类别，也就是子节点的纯度越高，我们就认为这个特征&amp;quot;好&amp;quot;。
为了量化子节点的纯度，前人提出了很多具体的评价方法，包括信息增益、信息增益率和基尼指数。这些评价方法大同小异，我们就以最常见的信息增益(Information Gain)为例进行解释：
假设当前节点包含的数据集为D，则数据集的熵(entropy)定义为：
其中\(|Y|\)表示类别值的取值个数。
假设某一个特征\(f\)有V个可能的取值 \(f^{1},f^{2},&amp;hellip;,f^{V}\), 若使用特征\(f\)对\(D\)进行划分，会产生\(V\)个子节点，其中第\(v\)个子节点包含了\(D\)中所有\(f\)取值为\(f^{v}\), 记作\(D^{v}\). 按照下式计算用\(f\)对\(D\)进行划分获得的信息增益(information gain):
信息增益越大，我们就认为用\(f\)划分所获得的纯度提升越大。
下面是用信息增益来构建决策树的简单Python代码，注意，构建的是二叉树。
def uniqueCounts(X):  &amp;#34;&amp;#34;&amp;#34; 对y的取值进行计数, 用于计算节点的purity rvalue: 计数结果 &amp;#34;&amp;#34;&amp;#34;  results = {}  for row in X:  y = X[-1]  results[y] = results.</description>
    </item>
    
    <item>
      <title>Bayesian Nets/Graph Model 画图工具</title>
      <link>https://basicv8vc.github.io/posts/bayesian-nets-graph-model-%E7%94%BB%E5%9B%BE%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Mon, 14 Sep 2015 15:23:19 +0000</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/bayesian-nets-graph-model-%E7%94%BB%E5%9B%BE%E5%B7%A5%E5%85%B7/</guid>
      <description>这几天在赶论文deadline，由于是第一次写论文，要学的真不少:(
论文和LDA有关，肯定少不了要在tex中插入LDA图模型，用什么工具画图呢？
tikz-bayesnet 一句话介绍bayesnet: TikZ library for drawing Bayesian networks, graphical models and (directed) factor graphs in LaTeX.
这个也是我这次使用的，语法非常简单。
bayesnet支持以下结点(node)类型:
 latent obs det const factor plate gate  使用时一共也就7类命令:
 \factor [options] {factorname}{caption}{inputs}{outpus} \plate [options] {platename}{fitlist}{caption} \gate [options] {gatename}{fitlist}{inputs} \vgate {vgatename}{fitlist-left}{caption-left}{fitlist-right}{caption-right}{inputs} \hgate {hgatename}{fitlist-top}{caption-top}{fitlist-bottom}{caption-bottom}{inputs} \edge [options] {inputs}{outputs} \factoredge [options] {inputs}{factors}{outputs}  看到这里，你可能会被options, caption, fitlist等各种参数绕晕，别急，我们来看一个例子，使用时非常简单滴。
让我们来画一个LDA流程图。
{% codeblock lang:latex %}  \documentclass[11pt]{report} \usepackage{tikz} \usetikzlibrary{bayesnet} \begin{document}  \begin{figure}  \centering  \tikz{ % 	\node [latent](beta){$\beta$}; % 	\node [latent, right=1.</description>
    </item>
    
  </channel>
</rss>
