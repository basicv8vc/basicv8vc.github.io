<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Yet Another Blog</title>
    <link>https://basicv8vc.github.io/posts/</link>
    <description>Recent content in Posts on Yet Another Blog</description>
    <image>
      <url>https://basicv8vc.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://basicv8vc.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 26 Jul 2022 23:17:00 +0000</lastBuildDate><atom:link href="https://basicv8vc.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>面向PyTorch用户的JAX简易教程[3]: 如何通过pmap轻松实现数据并行</title>
      <link>https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-3/</link>
      <pubDate>Tue, 26 Jul 2022 23:17:00 +0000</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-3/</guid>
      <description>背景 在上一篇文章中，我们学习了如何使用JAX+Flax+Optax训练神经网络。但是考虑到每块Cloud TPU上有8个core/device，而我们只用了一个device，
 好在我们的模型规模没有夸张到一张卡放不下，很自然的想到使用数据并行 (data parallelism, DP) 的方式来训练模型。
数据并行 ：假设有\( N \)张卡，每张卡都保存一个模型，每一次迭代（iteration/step）都将batch数据分割成\( N \)个等大小的micro-batch，每张卡根据拿到的micro-batch数据独立计算梯度，然后调用AllReduce计算梯度均值，每张卡再独立进行参数更新。
 数据并行流程   注意，本文的数据并行仅限在单机多卡环境，以后如果有多机资源会进行update。
pmap+jax.lax.p* 在单机多卡上轻松实现数据并行 pmap JAX中的pmap (parallel map) 让数据并行的实现方式异常简单，先来看一个简单的pmap示例，
import jax from jax import pmap, numpy as jnp  key = jax.random.PRNGKey(0)  # 定义一个函数，做向量点积 def f(x, y):  return jnp.dot(x, y)  # 创建两个向量x, y key, init_key1, init_key2 = jax.random.split(key, 3) x = jax.random.normal(init_key1, (10, )) y = jax.random.normal(init_key2, (10, ))  x.</description>
    </item>
    
    <item>
      <title>面向PyTorch用户的JAX简易教程[2]: 如何训练一个神经网络</title>
      <link>https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-2/</link>
      <pubDate>Sat, 23 Jul 2022 00:07:00 +0000</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-2/</guid>
      <description>背景 上一篇文章我们学习了JAX的基本知识，主要是几个关键词: NumPy API、transformations、XLA。这一篇来点实际的，看下如何训练一个神经网络，我们先回忆下用PyTorch训练神经网络，都需要哪几步：
 实现网络模型 实现数据读取流程 使用优化器/调度器更新模型参数/学习率 实现模型训练和验证流程  下面我们就以在MNIST数据集上训练一个MLP为例，看下在JAX中如何实现上面的流程。
NumPy API实现网络模型 MNIST是一个10分类问题，每张图片大小是 28 * 28=784 ，我们设计一个简单的MLP网络，
 一个四层MLP (包含输入层)   import jax from jax import numpy as jnp from jax import grad, jit, vmap  # 创建 PRNGKey (PRNG State) key = jax.random.PRNGKey(0)   ## 创建模型参数, 去除输入层，实际上三层Linear，每层都包含一组(w, b)，共三组参数  def random_layer_params(m, n, key, scale=1e-2):  &amp;#34;&amp;#34;&amp;#34; A helper function to randomly initialize weights and biases for a dense neural network layer &amp;#34;&amp;#34;&amp;#34;  w_key, b_key = jax.</description>
    </item>
    
    <item>
      <title>面向PyTorch用户的JAX简易教程[1]: JAX介绍</title>
      <link>https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-1/</link>
      <pubDate>Thu, 21 Jul 2022 01:31:00 +0000</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/jax-tutorials-for-pytorchers-1/</guid>
      <description>背景 前几天申请参加了Google TRC项目，TPU VM的配置相当可以，但是PyTorch/XLA做数据并行时的体验却并不那么丝滑，考虑到Google一直力推TPU+JAX的组合，所以决定学习下JAX。
JAX简介 什么是JAX？
官方在GitHub README中是这么介绍的: JAX is Autograd and XLA, brought together for high-performance machine learning research.
在Description中写的是: Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more.
在JAX官方文档又是这么介绍的: JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.
总结一下，有几个关键词：Autograd、XLA、NumPy和composable transformations。
XLA 先来说XLA，这个我了解的最少，所以介绍起来最简单，XLA (Accelerated Linear Algebra)是Google为TensorFlow设计的一款编译器，主打JIT (Just-in-Time)编译和跨设备（CPU/GPU/TPU）执行，所以JAX介绍中凡是涉及到JIT、high-performance、CPU/GPU/TPU，都指的是XLA。
NumPy NumPy就不用提了，Python生态下只要涉及到数据分析/机器学习/数值计算中对数组/tensor进行操作，都离不开它，不夸张的说，NumPy API已经成为了数组/tensor操作的半个工业标准，包括各家深度学习框架中对tensor操作的函数接口也都是尽量靠近NumPy，JAX则更夸张，jax.numpy重新实现一套了NumPy API ，让用户从NumPy无缝切入JAX：
from jax import numpy as jnp Autograd 这里的Autograd是哈佛大学HIPS实验室在14年开始开发的一款自动微分框架，特点是可以对Python/NumPy函数进行高阶求导，直接看个例子，一个简单的函数 f(x) ，顺便求一下一阶、二阶、三阶导函数：</description>
    </item>
    
    <item>
      <title>如何申请Google TRC项目，领取免费的Cloud TPU计算资源</title>
      <link>https://basicv8vc.github.io/posts/trc-program/</link>
      <pubDate>Thu, 07 Jul 2022 00:30:29 +0800</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/trc-program/</guid>
      <description>背景 当下，深度学习要想搞的好，算力、数据和模型哪一样都少不了。后两者相对来说比较容易解决，各种模型开源实现和公开数据集还是挺充足的，但是算力≈金钱，有时候脑海中蹦出来一些新鲜idea，可是看着手里的老旧显卡，只能在夜晚暗自神伤
 额，画风好像有点不对，但是大概就是这么个意思   一种方法是租显卡，比如某A、某B或者某C，如果是个人使用，不必像公司做项目那样顾虑太多，可以挑一些小公司的产品，有的还是挺便宜的。
另一种就是找免费的计算资源了
 比如Google Colab或者Kaggle，以及今天要介绍的Google Research的TPU Research Cloud (TRC)项目。
TRC项目  TRC项目中的T，指的是Google自家的加速卡TPU，和GPU不同，Google并不公开出售TPU设备，而是集成在Google Cloud中，提供挂载了TPU的云计算服务。
TRC项目就是Google免费赠送给我们一段时间的TPU服务器，比如这是我申请成功后的结果，5台Cloud TPU v2-8和5台Cloud TPU v3-8，以及100台抢占式的Cloud TPU v2-8。免费使用时间是60天。
 随便看一下其中一台的配置，
 CPU 96核，内存335GB，而且还挂载了TPU v2-8或者v3-8。我只能说，
 Cloud TPU TPU是Google推出的专用于机器学习的加速设备，可以类比NVIDIA的GPU，目前TPU已经更新到了第四代，也就是TPU v4，TRC项目提供的是前两代，TPU v2和TPU v3，对于大部分场景已经绰绰有余了。
简单说一下TPU，每块TPU上面有4块芯片（chip），每块芯片有两个核（core），所以这就是为什么叫做v2-8/v3-8，以v3为例，每个核有2个独立的矩阵计算单元（Matrix Multiply Unit, MXU）、1个向量处理单元（Vector Processing Unit, VPU）以及1个标量单元，每块芯片有32GB的高速存储（HBM）。
   所以，可以把v3-8简单理解为就是8张GPU卡。
除此之外，还有算力恐怖的TPU Pod，也就是几百上千块TPU组成的算力单元，当然，这个并没有免费开放给所有人。
TRC申请 由于Cloud TPU是以Google Cloud中的一种服务提供出来，所以先要有一个Google Cloud账号，然后再申请TRC项目。
Google Cloud开通 Google Cloud申请流程，网上有很多资料，这里就不细讲了，必要条件是有一个gmail邮箱、一个手机号以及一张支持VISA或MasterCard的信用卡。
// 貌似这一步是最难的:(
注册成功后，Google Cloud会赠送300$，有效期90天。
TRC申请 如果你开通了Google Cloud，接下来就可以申请TRC项目了，申请流程非常简单，就是填写一个小问卷：
   问题很简单，如实填写就行了，貌似就没有被拒的:)</description>
    </item>
    
    <item>
      <title>图解OneFlow中的学习率调整策略</title>
      <link>https://basicv8vc.github.io/posts/oneflow-lrscheduler-visualization/</link>
      <pubDate>Sun, 05 Jun 2022 22:50:29 +0800</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/oneflow-lrscheduler-visualization/</guid>
      <description>背景 学习率调整策略（learning rate scheduler），其实单独拎出每一个来看都不难，但是由于方法较多，上来就看文档容易一头雾水， 以OneFlow v0.7.0为例，oneflow.optim.lr_scheduler模块中就包含了14种策略。
有没有一种更好的方法来学习呢？比如可视化出学习率的变化过程，此时，我脑海中突然浮现出Convolution arithmetic这个经典项目，作者将各种CNN卷积操作以gif形式展示，一目了然。
 所以，就有了这篇文章，将学习率调整策略可视化出来，下面是两个例子（ConstantLR和LinearLR）：
  我将可视化代码分别托管在Hugging Face Spaces和Streamlit Cloud，建议大家访问第一个链接，然后自由调节参数，感受学习率的变化过程。
Hugging Face Space版本
Streamlit Cloud版本
学习率调整策略 学习率可以说是训练神经网络过程中最重要的参数（之一），目前大家都已接受用动态学习率调整策略来代替固定学习率，各种学习率调整策略层出不穷，下面我们就以OneFlow v0.7.0为例，学习下常用的几种策略。
基类LRScheduler LRScheduler(optimizer: Optimizer, last_step: int = -1, verbose: bool = False)是所有学习率调度器的基类，初始化参数中last_step和verbose一般不需要设置，前者主要和checkpoint相关，后者则是打印当前学习率。LRScheduler中最重要的方法是step()，这个方法的作用就是修改用户设置的初始学习率，然后应用到下一次的Optimizer.step()。
有些资料会讲LRScheduler根据epoch或iteration/step来调整学习率，两种说法都没问题，实际上，LRScheduler并不知道当前训练到第几个epoch或第几个iteration/step，只记录了调用step()的次数（last_step），如果每个epoch调用一次，那就是根据epoch来调整学习率，如果每个mini-batch 调用一次，那就是根据iteration来调整学习率，以Transformer为例，使用的是后者。
简单来说，LRScheduler根据调整策略本身、当前调用step()的次数（last_step）和用户设置的初始学习率来得到下一次梯度更新时的学习率。
ConstantLR oneflow.optim.lr_scheduler.ConstantLR(  optimizer: Optimizer,  factor: float = 1.0 / 3,  total_iters: int = 5,  last_step: int = -1,  verbose: bool = False ) ConstantLR和固定学习率差不多，唯一的区别是在前total_iters，学习率为初始学习率 * factor。
注意：由于factor取值[0, 1]，所以这是一个学习率递增的策略。</description>
    </item>
    
    <item>
      <title>DeepSpeed之ZeRO系列：将显存优化进行到底</title>
      <link>https://basicv8vc.github.io/posts/zero/</link>
      <pubDate>Sat, 14 May 2022 23:20:54 +0800</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/zero/</guid>
      <description>前言 目前训练超大规模语言模型主要有两条技术路线：TPU + XLA + TensorFlow/JAX 和 GPU + PyTorch + Megatron-LM + DeepSpeed。前者由Google主导，由于TPU和自家云平台GCP深度绑定，对于非Googler来说， 只可远观而不可把玩，后者背后则有NVIDIA、Meta、MS大厂加持，社区氛围活跃，也更受到群众欢迎。
上面提到的DeepSpeed的核心是ZeRO(Zero Redundancy Optimizer)，简单来说，它是一种显存优化的数据并行(data parallelism, DP)方案。而“优化“这个话题又永无止境，在过去两年DeepSpeed团队发表了三篇ZeRO相关的论文，提出了去除冗余参数、引入CPU和内存、引入NVMe等方法，从始至终都围绕着一个目标：将显存优化进行到底。
ZeRO: 一种去除冗余的数据并行方案 ZeRO: Memory Optimizations Toward Training Trillion Parameter Models 发表在SC 20，DeepSpeed项目最初就是论文中ZeRO方法的官方实现。
背景 如今训练大模型离不开各种分布式并行策略，常用的并行策略包括：
 数据并行（data parallelism, DP）：假设有 \(N\) 张卡，每张卡都保存一个模型，每一次迭代（iteration/step）都将batch数据分割成 \(N\) 个等大小的micro-batch，每张卡根据拿到的micro-batch数据独立计算梯度，然后调用AllReduce计算梯度均值，每张卡再独立进行参数更新。  # https://huggingface.co/docs/transformers/parallelism#model-parallelism # 假设模型有三层：L0, L1, L2 # 每层有两个神经元 # 两张卡  GPU0: L0 | L1 | L2 ---|----|--- a0 | b0 | c0 a1 | b1 | c1  GPU1: L0 | L1 | L2 ---|----|--- a0 | b0 | c0 a1 | b1 | c1  模型并行（model parallelism/tensor parallelism, MP/TP）：有的tensor/layer很大，一张卡放不下，将tensor分割成多块，一张卡存一块。  # https://huggingface.</description>
    </item>
    
    <item>
      <title>Hugging Face🤗 : NLP明星创业公司是如何炼成的</title>
      <link>https://basicv8vc.github.io/posts/huggingface/</link>
      <pubDate>Thu, 21 Apr 2022 19:09:35 +0800</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/huggingface/</guid>
      <description>2016年，两位法国人怀揣着对聊天机器人(chatbot)行业未来的美好向往，创立了Hugging Face 🤗，次年推出了一款和公司同名的机器人聊天App。最初，他们只是想为无聊的年轻人提供一个打发时间的地方，&amp;quot;Its entire purpose is to be fun&amp;quot; 可是不论是从技术难度还是商业模式上看，在2016年的时间点选择做开放领域的闲聊机器人，都不是一个明智选择。 时间来到了2018年10月29号，这一天公司首席科学家Thomas Wolf在GitHub上创建了一个名为pytorch-pretrained-BERT的项目，提交了第一个commit，这个项目最初的目的是如此简单，将Google基于tensorflow实现的BERT模型用pytorch进行了重写，并且可以加载Google公开的模型参数。但是，项目的火热程度超出了所有人的预期，Hugging Face也在开源模式中一路高歌猛进，顺利完成了公司转型，成为了当下NLP领域最火的创业公司（之一）。
入局聊天机器人 现在当你打开Hugging Face的官网，映入眼帘的都是&amp;quot;AI community&amp;quot;、&amp;ldquo;SOTA models&amp;rdquo;、&amp;ldquo;Open Source&amp;quot;等词汇，或许很多人都不知道，这曾经是一家做聊天机器人(chatbot)的公司，而且做的还是开放领域(open domain)的闲聊机器人，对，就是类似微软小冰，让我们把时间线拨回2016年。
Clem Delangue1和Julien Chaumond2一起参加了风投机构Betaworks举办的Botcamp加速器项目，专门为聊天机器人创业公司提供指导，在此期间（或者更早？）他们俩创立了Hugging Face3，研发一款为青少年打发时间找点乐子的陪聊型机器人，不论是公司名字（来自于emoji表情）还是产品，画风都既不商业(Enterprise)也不严肃，从这段时间的PR来看，他们坚信AI friend是大有未来的，We really have this vision where we think everyone will have an AI friend and everyone will discuss things every day with Hugging Face, so that’s really what we’re focused on right now and for the next few years4
 左边 是CTO Julien Chaumond，右边 是CEO Clem Delangue   在2017年3月9号，他们在iOS App Store正式推出了Hugging Face App，产品的界面是这样的：</description>
    </item>
    
    <item>
      <title>K-Means从入门到调包</title>
      <link>https://basicv8vc.github.io/posts/k-means%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%B0%83%E5%8C%85/</link>
      <pubDate>Tue, 19 Jul 2016 11:12:58 +0000</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/k-means%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%B0%83%E5%8C%85/</guid>
      <description>在做数据挖掘项目时，拿到数据，首先要分析问题和观察数据，我在“观察数据”这一阶段主要是看看有没有直观的数据规律以及异常点，对于那些特征维度较低的数据，比如仅仅2维，我选择画散点图观察，高纬度数据可以用t-SNE降维后再画图。除了可视化，聚类算法也是找到异常点的常用手段之一。
这篇博客介绍一种最基本的聚类算法：K-Means。
算法描述与EM 聚类算法的作用是将数据集自动划分为多个不同的集合。 K-Means算法是典型的无监督算法，对于数据集\(D\), 我们设定参数k(k个集合)，她就能自动地学习出k个类别，简单又神奇。那么K-Means是如何学习到这k个集合呢？专业一点说就是，K-Means的目标函数是啥？如何优化她的目标函数？先回答第一个问题，下面就是K-Means的目标函数：
物理意义：我们希望找到的k个集合，每个集合内的点都距离此集合内平均值点最近(每个集合中的点都紧密团结在一起)。
知道了目标函数，下一步就要考虑如何求解，最常见的求解方法是Lloyd算法，也就是使用EM算法，注意，Lloyd算法得到的结果是局部极小值而不是全局最小值哦。
EM算法求解步骤很简单，总共分三步：
 将所有数据点都分配到某个集合 -&amp;gt; 计算每个点到集合平均值点的欧氏距离 计算每个集合内点的平均值 -&amp;gt; 计算每个特征的平均值 重复上面两步直到局部收敛 -&amp;gt; 前后两次迭代的集合结果相同  上面的三步概括起来就是 E-&amp;gt;M-&amp;gt;E-&amp;gt;M-&amp;gt;&amp;hellip;&amp;hellip;&amp;hellip;..
import numpy as np from collections import defaultdict from random import uniform class KMeans(object): &amp;#34;&amp;#34;&amp;#34; kmeans 聚类算算 &amp;#34;&amp;#34;&amp;#34; def __init__(self): pass def cluster(self, X, k, initializer=&amp;#34;random&amp;#34;): self.X = X self.k = k self.centers = defaultdict(list) self.assignment = np.zeros(self.X.shape[0]) #记录每个样本属于哪个中心点 if initializer == &amp;#34;random&amp;#34;: self._initialize() #初始化得到k个点 elif initializer == &amp;#34;kmeans++&amp;#34;: self.</description>
    </item>
    
    <item>
      <title>Gradient Boosting 从入门到调包 (调包篇)</title>
      <link>https://basicv8vc.github.io/posts/gradient-boosting-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%B0%83%E5%8C%85-%E8%B0%83%E5%8C%85%E7%AF%87/</link>
      <pubDate>Sat, 09 Jul 2016 21:36:18 +0000</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/gradient-boosting-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%B0%83%E5%8C%85-%E8%B0%83%E5%8C%85%E7%AF%87/</guid>
      <description>Gradient Boosting的开源实现很多，当然首选还是XGBoost。调包，就是学习API的过程，最好的学习资料就是官方文档以及demo。我这里把基本的调用罗列出来:
xgboost调包之 基础篇.ipynb
xgboost调包之 模型篇.ipynb
xgboost调包之 sklearn接口.ipynb
xgboost调包之 特征重要性可视化.ipynb
xgboost调包之 early stopping.ipynb
xgboost调包之 结合pandas.ipynb</description>
    </item>
    
    <item>
      <title>Gradient Boosting 从入门到调包 (入门篇)</title>
      <link>https://basicv8vc.github.io/posts/gradient-boosting-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%B0%83%E5%8C%85-%E5%85%A5%E9%97%A8%E7%AF%87/</link>
      <pubDate>Fri, 08 Jul 2016 21:36:18 +0000</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/gradient-boosting-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%B0%83%E5%8C%85-%E5%85%A5%E9%97%A8%E7%AF%87/</guid>
      <description>开篇 虽然数据挖掘算法很多，但有过实际经验的同学肯定都知道，集成方法(ensembel method)大多数情况下都是首选。集成方法和LR、SVM算法不同，她不是一个具体的模型或算法，而是一类算法的统称。集成，从其字面意思也能看出，这家伙不是单兵作战，而是依靠团队取胜。简而言之，集成方法就是通过构建并结合多个学习器来完成学习任务。这句话有两个地方需要注意，“如何构建每一个学习器”以及“如何结合多个学习器而产生预测结果”。
如果我们按照“如何构建每一个学习器”来将集成方法分类，总体可以分为两类：1) 个体学习器减存在强依赖关系、必须串行生成的序列化方法，代表方法是Boosting；2）个体学习器之间不存在强依赖关系、可同时生成的并行化方法，代表方法是Bagging。
本文要讲到的Gradient Boosting就属于Boosting方法。
由于Gradient Boosting涉及到的知识实在是很多，完全可以写成类似&amp;quot;Understanding Random Forest: From Theory to Practice&amp;quot;的博士论文，由于水平和时间的限制，本文不会涉及过多、过深的细节，而是尽量用通俗的自然语言和数学语言来解释我对Gradient Boosting的浅显理解，仅仅是入门水平:)
决策树 既然要讲Gradient Boosting，那么决策树肯定是绕不开的，虽然Boosting方法对&amp;quot;个体学习器&amp;quot;没有要求，你可以选择LR、SVM、决策树等等。但是大多数情况下我们默认都选择决策树作为个体学习器。下面简单解释决策树算法。
决策树，顾名思义，根据树结构来进行决策。这种思想是很质朴的，和我们人类做决策时的思考过程很像，比如明天出门要不要带伞啊？如果我根据天气情况来做出决定，可以记作：
if 明天下雨： 出门带伞 else: 出门不带伞 看到没有，决策过程可以用if, else来具体化，难怪有人说，决策树就是一堆if else而已。数据挖掘比赛中很多人会首先用简单的规则跑出一个结果，这实际上就是在人工构造决策树啊。
既然决策树是一种机器学习方法，我们就想知道她的学习过程是怎么样的，也就是给定训练集如果构建一颗决策树呢？通常，决策树的构建是一个递归过程:
 1 输入训练集D，特征集F 2 如果D中样本全属于同一个类别C，将当前节点标记为C类叶节点，返回 3 从F中选择最优划分特征f 4 对于f的每一个值\(f_i\), 都生成一个子节点，然后对D进行划分，f值相同的样本都放到同一个子节点，并对节点进行标记，然后对于每一个新产生的子节点，进行第1步  注意：实际上对于决策树，大多数的实现方法都是基于二叉树，比如sklearn。
划分选择 决策树算法的关键是上面的第3步，即如何选择最优划分特征。如何评价一个特征是不是优秀呢？基本的原则就是看按照某一个特征生成子节点后，子节点的样本是不是都尽可能属于同一个类别，也就是子节点的纯度越高，我们就认为这个特征&amp;quot;好&amp;quot;。
为了量化子节点的纯度，前人提出了很多具体的评价方法，包括信息增益、信息增益率和基尼指数。这些评价方法大同小异，我们就以最常见的信息增益(Information Gain)为例进行解释：
假设当前节点包含的数据集为D，则数据集的熵(entropy)定义为：
其中\(|Y|\)表示类别值的取值个数。
假设某一个特征\(f\)有V个可能的取值 \(f^{1},f^{2},&amp;hellip;,f^{V}\), 若使用特征\(f\)对\(D\)进行划分，会产生\(V\)个子节点，其中第\(v\)个子节点包含了\(D\)中所有\(f\)取值为\(f^{v}\), 记作\(D^{v}\). 按照下式计算用\(f\)对\(D\)进行划分获得的信息增益(information gain):
信息增益越大，我们就认为用\(f\)划分所获得的纯度提升越大。
下面是用信息增益来构建决策树的简单Python代码，注意，构建的是二叉树。
def uniqueCounts(X):  &amp;#34;&amp;#34;&amp;#34; 对y的取值进行计数, 用于计算节点的purity rvalue: 计数结果 &amp;#34;&amp;#34;&amp;#34;  results = {}  for row in X:  y = X[-1]  results[y] = results.</description>
    </item>
    
    <item>
      <title>Bayesian Nets/Graph Model 画图工具</title>
      <link>https://basicv8vc.github.io/posts/bayesian-nets-graph-model-%E7%94%BB%E5%9B%BE%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Mon, 14 Sep 2015 15:23:19 +0000</pubDate>
      
      <guid>https://basicv8vc.github.io/posts/bayesian-nets-graph-model-%E7%94%BB%E5%9B%BE%E5%B7%A5%E5%85%B7/</guid>
      <description>这几天在赶论文deadline，由于是第一次写论文，要学的真不少:(
论文和LDA有关，肯定少不了要在tex中插入LDA图模型，用什么工具画图呢？
tikz-bayesnet 一句话介绍bayesnet: TikZ library for drawing Bayesian networks, graphical models and (directed) factor graphs in LaTeX.
这个也是我这次使用的，语法非常简单。
bayesnet支持以下结点(node)类型:
 latent obs det const factor plate gate  使用时一共也就7类命令:
 \factor [options] {factorname}{caption}{inputs}{outpus} \plate [options] {platename}{fitlist}{caption} \gate [options] {gatename}{fitlist}{inputs} \vgate {vgatename}{fitlist-left}{caption-left}{fitlist-right}{caption-right}{inputs} \hgate {hgatename}{fitlist-top}{caption-top}{fitlist-bottom}{caption-bottom}{inputs} \edge [options] {inputs}{outputs} \factoredge [options] {inputs}{factors}{outputs}  看到这里，你可能会被options, caption, fitlist等各种参数绕晕，别急，我们来看一个例子，使用时非常简单滴。
让我们来画一个LDA流程图。
{% codeblock lang:latex %}  \documentclass[11pt]{report} \usepackage{tikz} \usetikzlibrary{bayesnet} \begin{document}  \begin{figure}  \centering  \tikz{ % 	\node [latent](beta){$\beta$}; % 	\node [latent, right=1.</description>
    </item>
    
  </channel>
</rss>
